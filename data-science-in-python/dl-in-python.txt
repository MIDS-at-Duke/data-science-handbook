
#### deep learning with Keras

	### Steps in keras modelling
			# 0. Preprocess data
			# 1. Create architecture
			# 2. Compile model
			# 3. Fit model
			# 4. Evaluate and predict using model

==============================================================================================================================
	
	### multilayer perceptron

		## 0. preprocess

			# get X_train, y_train, X_test, y_test using sklearn preprocessing pipeline

			# one-hot encode the outcome variable - in case of multiclass classification problem
			from keras.utils import to_categorical
			y_train   = to_categorical(y_train)

			# preprocess test data in the exact same way as train data

			n_features = X_train.shape[1]
			n_classes  = y_train.shape[1]                             # multiclass classification

		## 1. create architecture

			from keras.models import Sequential
			from keras.layers import Dense

			# instantiate model
			model = Sequential()

			# add first hidden layer
			from keras import backend as K
			activation = "tanh" or K.tanh or K.relu(x, alpha = 0.0, max_value = None)  # refer keras activations 
			
			from keras.initializers import glorot_normal
			initializer = "glorot_normal" or glorot_normal(seed = 4)  # refer keras initializers for creating new regularizer
			
			from keras.regularizers import l1
			regularizer = l1(0.01)                                    # refer keras regularizers for creating new regularizer

			model.add(Dense(units                 =   n_neurons,      # number of neurons in the layer     
							activation            =   activation,     # activation function. None means 'linear' activation
							input_shape           =   (n_features,),  # mention this for the first hidden layer only
							use_bias              =   True,           # whether the layer uses bias vector or not
							kernel_initializer    =   initializer,    # weights initializer
							bias_initializer      =   initializer,    # bias initializer
							kernel_regularizer    =   regularizer,    # use regularization on weight matrix
							bias_regularizer      =   regularizer,    # use regularization on bias vector
							activity_regularizer  =   regularizer,    # use regularization on output of activation
							kernel_constraint     =   ,               # restrict the weight matrix values 
							bias_constraint       =                   # restrict the bias vector values
			))
			
			# add activation - if not added as an argument to Dense()
			from keras.layers import Activation                       # for simple activations
			from keras.layers.advanced_activations import LeakyReLU   # for advanced activations
			
			# simple activation
			model.add(Activation(activation))
			
			OR
			
			# advanced activation			
			model.add(LeakyReLU(alpha=0.3))                           # figure which is the right way

			# normalize the layer
			from keras.layers import BatchNormalization
			model.add(BatchNormalization())                           # apply it before activation layer

			# regularization techniques - the following are applied to previous layer
			model.add(GaussianNoise(stddev = 1))                      # add zero-centered gaussian noise to neuron values with std
			model.add(Dropout(rate = 0.2, seed = 4))                  # drop 20% neurons
			model.add(GaussianDropout(rate = 0.2))                    # drop 20% neurons and multiply neurons with one-centered gaussian noise 

			# add second hidden layer
			model.add(Dense(n_neurons, activation = ""))

			# add last layer
			model.add(Dense(1, activation = "linear"))                # for regression
			model.add(Dense(1, activation = "sigmoid"))               # for binary classification - use binary crossentropy loss
			model.add(Dense(n_classes, activation = "softmax"))       # for multiclass classification - use categorical crossentropy loss

		## 2. compile model

			from keras.optimizers import SGD
			optimizer = 'sgd' or SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)  # refer keras optimizers

			from keras import losses
			loss = losses.mean_squared_error/losses.binary_crossentropy/losses.categorical_crossentropy  # refer keras losses

			from keras import metrics
			metrics = [metrics.mean_squared_error, metrics.binary_accuracy, metrics.categorical_accuracy, metrics.top_k_categorical_accuracy(k = 5)]  # refer keras metrics

			model.compile(optimizer  =   optimizer,
						  loss       =   loss,
						  metrics    =   metrics
			)

			# model summary
			from keras.utils import print_summary
			print_summary(model, line_length = None, positions = None, print_fn = None)

			from keras.utils import plot_model
			plot_model(model, to_file = "model.png", show_shapes = False, show_layer_names = True, rankdir = "TB/LR")

			from IPython.display import SVG
			from keras.utils.vis_utils import model_to_dot
			SVG(model_to_dot(model).create(prog='dot', format='svg')) # google - how to visualise pydot.Graph object

		## 3. fit model

			training = model.fit(x                =   X_train,
							 	 y                =   y_train,
				  			 	 batch_size       =   128,
				  				 epochs           =   10,
				  			 	 verbose          =   1,                     # 0, 1 or 2
							 	 callbacks        =   callbacks,             # refer miscellaneous
				  			 	 validation_split =   0.20,
				  			 	 validation_data  =   (X_test, y_test),      # pass validation_split or validation_data
				  			 	 shuffle          =   True,                  # shuffle data before each epoch
				  			 	 class_weight     =   None                   # assign weight in case of unbalanced class
			)
			
			model.summary()
			training.history.keys()

			# look at all the layers of the model
			print(model.layers)

		## 4. evaluate model

			# evaluate model performance on test set
			loss, accuracy = model.evaluate(X_test, y_test, verbose = 1)
			print("Loss: {0}, Accuracy: {1}".format(loss, accuracy))
			print(model.metrics_names)                                       # no idea what this function does
			
			# predict values on test set
			y_pred = model.predict(X_test)

			# visualise training history
			plt.plot(training.history['acc/loss'])
			plt.plot(training.history['val_acc/val_loss'])
			plt.title('model accuracy/loss')
			plt.ylabel('accuracy/loss')
			plt.xlabel('epoch')
			plt.legend(['train', 'test'], loc="upper left")
			plt.show()

==============================================================================================================================

	### cnn

		## 0. preprocess

			# get preprocessed X_train, y_train, X_test, y_test
			X_train, y_train, X_test, y_test = mnist.load_data()

			# reshape X - (samples, height, width, channels)
			samples  = X_train.shape[0]
			height   = 28
			width    = 28
			channels = 3

			X_train  = X_train.reshape((samples, height, width, channels))
			X_train  = X_train.astype('float32')
			X_train  = X_train/255

			# one-hot encode y
			from keras.utils import to_categorical
			y_train   = to_categorical(y_train)
			n_classes = y_train.shape[1]

			# preprocess test data in the exact same way as train data
			X_test  = X_test.reshape((samples, height, width, channels))
			X_test  = X_test.astype('float32')
			X_test  = X_test/255

			y_test   = to_categorical(y_test)                              # for validation purpose - y_test won't be available in real test data


		## 1. create architecture

			from keras.layers import Conv2D
			from keras.layers import MaxPooling2D

			model = Sequential()
			model.add(Conv2D(filters               =   32,                 # number of filters
							 kernel_size           =   (5, 5),             # size of each filter
							 strides               =   (1, 1),             # stride size
							 padding               =   "valid/same",       # valid - no padding, same - keeps same dimensions
							 data_format           =   'channels_last',
							 input_shape           =   (height, width, channels),
							 activation            =   ,
							 use_bias              =   ,
							 kernel_initializer    =   ,
							 bias_initializer      =   ,
							 kernel_regularizer    =   ,
							 bias_regularizer      =   ,
							 activity_regularizer  =   ,
							 kernel_constraint     =   ,
							 bias_constraint       =
			))
			model.add(Conv2D())
			model.add(MaxPooling2D(pool_size = (2, 2)))
			model.add(Dropout(0.2))
			# followed by [m*(n*(CONV) ==> POOL ==> (REGULARISE)?) ==> flatten ==> k*dense] layers
			# number of filters keep increasing as CNN becomes deeper

			model.add(Conv2D())
			model.add(Conv2D())
			model.add(MaxPooling2D())
			model.add(Dropout(0.2))

			model.add(Flatten())                                           # change convolution layers to fully connected layer
			
			model.add(Dense(128))
			model.add(Dense(n_classes, activation='softmax'))


		## 2. compile model

		## 3. fit model

		## 4. evaluate model

		## transfer learning

==============================================================================================================================

	### rnn

		## 0. preprocess
			# data_shape = (samples, time_steps, n_features) # applies to both X and y

			# split data
			X_train, X_test, y_train, y_test = 

			# sample document - the following line just shows how a rows in train and test data looks like - one row = one document
			documents = ["This is document one.",
						 "This is document two. A document can contain multiple sentences!",
						 "This is the third document. :)"]

			# i) document cleaning
			def clean_document(document, remove_punct=True, sentence_case="lower", remove_stops=True, spell_correction=False, stem=True, min_word_length=0, char_filter = r"[^\w]"):
			    '''
				input:
					document          :  string
					remove_punct      :  whether to remove all the punctuations from the document
					sentence_case     :  change document to "lower" case, "upper" case, or keep "same" case as provided
					remove_stops      :  whether to remove stopwords from document
					spell_correction  :  whether to correct spelling of each word
					stem              :  whether to stem each word
					min_word_length   :  remove words shorter than min_word_length
					char_filter       :  regex pattern - removes those characters from the text that match the pattern

				output: clean document
			    '''

			    # remove all punctuations
			    from string import punctuation
			    if remove_punct:
			    	document = document.translate(punctuation)
			    
			    # convert words to lower case
			    if sentence_case == "lower":
			    	document = document.lower()
			    elif sentence_case == "upper":
			    	document = document.upper()

			    # tokenise words
			    from nltk.tokenize import word_tokenize
				words = word_tokenize(document)
			    
			    # strip whitespace from all words
			    words = [word.strip() for word in words]

			    # remove stopwords
			    from nltk.corpus import stopwords
			    if remove_stops:
			    	stops = set(stopwords.words("english"))
				    words = [word for word in words if word not in stops]

				# spell correction
				from spell_corrector import rectify
				if spell_correction:
					words = [rectify(word) for word in words]
				
				# stemming
				from nltk.stem.snowball import SnowballStemmer
				stemmer = SnowballStemmer('english')
				words = [stemmer.stem(word) for word in words]

			    # remove extremely short words
			    words = [word for word in words if len(word) > min_word_length]

			    # joing back words to get document
			    document = " ".join(words)

			    # remove unwanted characters
			    document = re.sub(char_filter, " ", document)       # compile regex for quick processing

			    # replace multiple whitespaces with single whitespace
			    document = re.sub(r"\s+", " ", document)

			    # strip whitespace from document
			    document = document.strip()
			    
			return document		

			X_train = training_data.sentence_column.apply(lambda x: clean_sentence(x), axis=1)
			
			# ii) convert text to integers
			from keras.preprocessing.text import Tokenizer
			tokenizer = Tokenizer(num_words   = top_n,                    # optional - number of words to keep, based on frequency
								  filter      = '',                       # characters to remove - default is all punctuation, plus tabs and line breaks
								  char_level  = False,                    # whether to character tokenise or word tokenise
								  oov_token   = None                      # out of vocabulary token
			)
			tokenizer.fit_on_texts(training_data)
			train_sequences = tokenizer.texts_to_sequences(X_train)

			# define constants for text data
			VOCABULARY_SIZE = 20000              # get vocabulary size either by using training data or by using Keras' Tokenizer API
			EMBEDDING_SIZE  = 300                # set embeddings size     - n_features
			MAX_SEQ_LENGTH  = 30                 # set the sequence length - time_steps

			# iii) pad document in order to have same length for each document
			from keras.preprocessing.sequence import pad_sequences
			train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQ_LENGTH)

			# one-hot encode y
			from keras.utils import to_categorical
			y_train   = to_categorical(y_train)
			n_classes = y_train.shape[1]

			# iv) prepare word embedding - refer 'word_embeddings.txt'


			# v) prepare test data in the same manner - clean the text, convert text to integers (using the train fit) and then pad
			X_test = test_data.sentence_column.apply(lambda x: clean_sentence(x), axis=1)
			test_sequences = tokenizer.texts_to_sequences(X_test)
			test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQ_LENGTH)

			y_test = to_categorical(y_test)                               # for validation purpose - y_test won't be available in real test data


		## 1. create architecture

			# import libraries
			from keras.models import Sequential
			from keras.layers import Dense
			from keras.layers import SimpleRNN
			from keras.layers import LSTM
			from keras.layers import GRU
			from keras.layers import Bidirectional
			from keras.layers import TimeDistributed
			from keras.layers import RepeatVector

			# hidden layer parameters
			n_cells      =  # number of neurons to add in the hidden layer
			time_steps   =  # length of sequences; MAX_SEQ_LENGTH in case of textual data; number of frames in case of videos
			n_features   =  # number of features of each entity in the sequence; VOCABULARY_SIZE in case of textual data; Price, Temperature and Pressure in case of time series data; height*width*channels in case of images or videos

			# output layer parameters
			n_classes         =  # 1 in case of regression and binary classification, number of classes in case of multiclass classification
			output_activation =  # "linear" in case of regression, "sigmoid" for binary classification, "softmax" for multiclass classification

			==================================================================================================================
			# vanilla RNN
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# add embedding layer in case of textual data
			model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data
								output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented
								input_length  =  MAX_SEQ_LENGTH,          # length of input sequence
								weights       =  [embedding_weights],     # word embedding matrix
								trainable     =  True                     # True - update embeddings_weight matrix
			))

			# add hidden RNN layer; skip input_shape if embedding layer is present
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features)))

			# add output layer
			model.add(Dense(n_classes, activation=output_activation)


			==================================================================================================================
			# stacked RNN
			==================================================================================================================

			# instantiate model
			model = Sequential()
			
			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# add first hidden RNN layer; return_sequences is True to give output at each timestep
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features), return_sequences=True))
			
			# add second hidden RNN layer
			model.add(SimpleRNN(n_cells, return_sequences=True))
			
			# add third hidden RNN layer
			model.add(SimpleRNN(n_cells, return_sequences=True))
			
			# add fourth hidden RNN layer
			model.add(SimpleRNN(n_cells))
			
			# add output layer
			model.add(Dense(n_classes, activation=output_activation)
			

			==================================================================================================================
			# bidirectional RNN
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# add hidden bidirectional RNN layer
			model.add(Bidirectional(SimpleRNN(n_cells, input_shape=(time_steps, n_features), merge_mode='concat')) 

			# add output layer
			model.add(Dense(n_classes, activation=output_activation))


			==================================================================================================================
			# LSTM network
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# add hidden LSTM layer
			model.add(LSTM(n_cells, input_shape=(time_steps, n_features)))

			# add output layer
			model.add(Dense(n_classes, activation=output_activation))


			==================================================================================================================
			# GRU network
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# add hidden GRU layer
			model.add(GRU(n_cells, input_shape=(time_steps, n_features)))

			# add output layer
			model.add(Dense(n_classes, activation=output_activation))


			==================================================================================================================
			# many-to-one RNN
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# many inputs: one input at each timestep
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features)))

			# one output: one output at the last timestep
			model.add(Dense(n_classes, activation=output_activation))


			==================================================================================================================
			# many-to-many RNN - input sequence is equal to output sequence
			==================================================================================================================

			# instantiate model
			model = Sequential()

			# many inputs: one input at each timestep; return_sequences is True to give output at each timestep
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features), return_sequences=True))

			# many outputs: one output at each timestep
			model.add(TimeDistributed(Dense(n_classes, activation=output_activation)))


			==================================================================================================================
			# encoder-decoder RNN: input sequence is not equal to output sequence
			==================================================================================================================

			# encoder parameters
			encoder_n_cells   =  # number of neurons to add in the hidden layer of the encoder RNN
			input_timesteps   =  # length of input sequences

			# decoder parameters
			decoder_n_cells   =  # number of neurons to add in the hidden layer of the decoder RNN
			output_timesteps  =  # length of output sequences

			# instantiate model
			model = Sequential()

			# encoder RNN: one input at each timestep of the encoder
			model.add(SimpleRNN(encoder_n_cells, input_shape=(input_timesteps, n_features)))

			# encoded sequence: output of encoder at the last timestep is repeated so that it could be fed at each timestep in the decoder RNN
			model.add(RepeatVector(output_timesteps))

			# decoder RNN with encoded sequence fed at every timestep; return_sequences is True to give output at each timestep
			model.add(SimpleRNN(decoder_n_cells, return_sequences=True))

			# many outputs: one output at each timestep of the decoder
			model.add(TimeDistributed(Dense(n_classes, activation=output_activation)))


			==================================================================================================================
			# one-to-many RNN
			==================================================================================================================

			time_steps = 1  # length of input sequence is one

			# instantiate model
			model = Sequential()

			# one input: one input at the first timestep; return_sequences is True to give output at each timestep
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features), return_sequences=True))

			# many outputs: one output at each timestep
			model.add(TimeDistributed(Dense(n_classes, activation=output_activation)))


			==================================================================================================================
			# stateful RNN: when order of training example is also important
			==================================================================================================================
			# https://keras.io/getting-started/faq/#how-can-i-use-stateful-rnns

			# define number fo epochs
			epochs = 10

			# instantiate model
			model = Sequential()
			
			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# add hidden RNN layer
			model.add(SimpleRNN(n_cells, stateful=True, batch_input_shape=(32, 10, 25)))
			
			# fit model
			for i in range(epochs):
				model.fit(X, y, epochs=1, shuffle=False, batch_input_shape=(32, 10, 25))
				model.reset_states()
			
			# make predictions in batches
			predictions = model.predict(X, batch_size=32)
			
			==================================================================================================================
			# CNN RNN
			==================================================================================================================

			# image_data_shape = (n_images, height, width, channels)
			time_steps =  1
			n_features =  height*width*channels
			
			# video_data_shape = (n_videos, n_frames, height, width, channels)
			time_steps =  n_frames
			n_features =  height*width*channels
			

			# instantiate model
			model = Sequential()

			# add embedding layer in case of textual data
			model.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH, weights=[embedding_weights], trainable=True))

			# CNN - 2D for images, 1D for text; TimeDistributed() in case of video
			model.add(TimeDistributed(Conv2D(...))
			model.add(TimeDistributed(MaxPooling2D(...)))
			model.add(TimeDistributed(Flatten()))

			# add hidden RNN layer 
			model.add(SimpleRNN(n_cells, input_shape=(time_steps, n_features), return_sequences=True))

			# add output layer
			model.add(TimeDistributed(Dense(n_classes, activation=output_activation)))


		## 2. compile model

		## 3. fit model

		## 4. evaluate model

==============================================================================================================================

	### miscellaneous

		## keras + sklearn

			# 1. Build function that defines, compiles and returns a keras model
			def create_model(activation = "sigmoid"):
				
				# create architecture
				model = Sequential()
				model.add(Dense(8))
				model.add(Dense(8))
				model.add(Dense(2, activation = activation))
				
				# compile model
				model.compile(loss = , optimizer = , metrics = )
				
				return model

			# 2. Create scikit-learn compatible model using keras wrapper
			from keras.wrappers.scikit_learn import KerasClassifier
			from keras.wrappers.scikit_learn import KerasRegressor
			model = KerasClassifier(build_fn = create_model, epochs = , batch_size = 10, verbose = 1)
			model = KerasRegressor(build_fn = create_model, epochs = , batch_size = 10, verbose = 1)

			# 3. Use model as a native scikit-learn model
			cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)

			OR

			optimizers = ['rmsprop', 'adam']
			weight_initializers = ['glorot_uniform', 'normal', 'uniform']
			epochs = [50, 100, 150]
			batches = [32, 64, 128]
			params = {"optimizer"           : optimizers,
					  "epochs"              : epochs,
					  "batch_size"          : batches,
					  "weight_initializer"  : weight_initializers
			}
			model_cv = GridSearchCV(estimator = model, param_grid = params)
			model_cv.fit(X_train, y_train)
			model_cv.cv_results_
			model_cv.best_score_
			model_cv.best_params_


		## callbacks
			
			# early stopping
			from keras.callbacks import EarlyStopping
			early_stopping = EarlyStopping(monitor     =  'val_loss',        # or 'val_acc'
										   min_delta   =  0,                 # improvement in score less than min_delta will not be considered as an improvement 
										   patience    =  3,                 # number of epochs to monitor before stopping training
										   verbose     =  0/1,               # verbosity
										   mode        =  'auto'             # maximise or minimise monitoring quantity before stopping training
			)

			# model checkpoint
			from keras.callbacks import ModelCheckpoint
			filepath = "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"  # multiple checkpoints as a result of different file names
			filepath = "weights-improvement.hdf5"                            # checkpoints will be overwritten to this file
			checkpoint = ModelCheckpoint(filepath,
										 monitor            =  'val_loss',   # or 'val_acc'
										 verbose            =  0/1,          # verbosity
										 save_best_only     =  True,         # don't overwrite the best model
										 save_weights_only  =  False,        # save weightsonly or entire model
										 mode               =  'auto',       # maximise or minimise the monitoring quantity before saving
										 period             =  1             # number of epochs between checkpoints
			)

			# learning rate scheduler
			from keras.callbacks import LearningRateScheduler
			lr_schedular = LearningRateScheduler(lambda x: 1. / (1. + x),    # pass a function which takes current epoch index and lr and outputs updated lr
												 verbose = 0/1)
			
			# reduce learning rate when loss becomes stagnant
			from keras.callbacks import ReduceLROnPlateau
			reduce_lr = ReduceLROnPlateau(monitor     =  'val_loss',
										  factor      =  0.1,                # reduce lr by this factor
										  patience    =  10,                 # lr is reduced if there is no improvement in monitor metric for 10 epochs
										  verbose     =  0/1,                # verbosity
										  mode        =  'auto',             # maximise or minimise the monitoring quantity before saving
										  min_delta   =  0.0001,             # improvement in score less than min_delta will not be considered as an improvement 
										  cooldown    =  0,                  # no idea about this
										  min_lr      =  0                   # minimum value of lr after which it's not reduced
			)

			# tensorboard
			from keras.callbacks import TensorBoard
			tensorboard = TensorBoard(log_dir                 =  './logs',   # log directory
									  histogram_freq          =  1,          # frequency (in epochs) at which to compute activation and weight histograms for the layers of the model
									  batch_size              =  128,        # size of batch of inputs to feed to the network for histograms computation
									  write_graph             =  True,       # whether to visualize the graph 
									  write_grads             =  True,       # whether to visualize gradient histograms
									  write_images            =  False,      # whether to write model weights to visualize as image
									  embeddings_freq         =  0,          # frequency (in epochs) at which selected embedding layers will be saved
									  embeddings_layer_names  =  None        # list of names of layers to keep an eye on. If None then watch all embedding layers
			)
			
			# combine all callbacks and pass to fit method
			callbacks = [early_stopping, checkpoint, lr_schedular, reduce_lr, tensorboard]

		## model attributes

			# save model
			model.save("my_model.h5")
			
			# load model
			from keras.models import load_model
			model = load_model("my_model.h5")
