==============================================================================================================================

### R

        # arithmetic
        +   # addition
        -   # subtraction
        *   # multiplication
        /   # division
        ^   # exponentiation
        %%  # modulo

        # data types
        runs      <- 84       # numeric
        greeting  <- "hello"  # character
        is_r_cool <- TRUE     # boolean - TRUE and FALSE

        class(item)           # get data type of item
        identical(x, y)       # return TRUE only if object x is exactly equal to object y

        # vectors

            # create vector
            my_vector <- c(1, 2, 3)      # numeric vector
            my_vector <- c(TRUE, FALSE)  # boolean vector
            my_vecotr <- c("a", "b")     # character vector
            my_vector <- c(1, TRUE)      # Throws error! Vectors can hold one only data type

            my_vector <- numeric(10)     # create empty numeric vector of length ten with zeros
            my_vector <- character(10)   # create empty character vector of length ten with ""
            my_vector <- logical(10)     # create empty logical vector of length ten with FALSE

            # vector functions
            names(my_vector)   <- c("ID", "Gender")  # assign names to vector
            vector_arithmetic  <- a + b              # operation takes elementwise
            vector_sum         <- sum(my_vector)
            vector_length      <- length(my_vector)
            vector_mean        <- mean(my_vector)
            vector_median      <- median(my_vector)
            vector_sd          <- var(my_vector)
            vector_var         <- sd(my_vector)

            # subsetting vectors using indices
            element  <- my_vector[3]          # extract third element
            elements <- my_vector[c(2,3,4)]   # extract element at index 2, 3 and 4
            elements <- my_vector[-c(2,3,4)]  # extract all elements except the ones at indices 2, 3 and 4
            elements <- my_vector[2:10]       # extract elements from 2 through 10 inclusive

            # subsetting using boolean
            boolean   <- my_vector > 0       # creates a boolean vector by checking if every element is greater than zero
            my_vector <- my_vector[boolean]  # returns only those elements which are TRUE
            OR
            my_vector <- my_vector[my_vector > 0]

        # matrices

            # create matrix
            my_matrix <- matrix(1:9, byrow=TRUE, nrow=3)
            OR
            v1 <- c(1,2)
            v2 <- c(1,2)
            v2 <- c(1,2)
            elements  <- c(v1, v2, v3)  # results in c(1, 2, 1, 2, 1, 2)
            my_matrix <- matrix(elements, byrow=TRUE, nrow=2)

            # elementwise matrix arithmetic
            matrix_arithmetic <- a + b
            matrix_arithmetic <- A/5

            # matrix multiplication
            matrix_product <- a %*% b

            # matrix functions
            rownames(my_matrix) <- c(1,2,3)          # set row names
            colnames(my_matrix) <- c("A", "B", "C")  # set column names
            row_sum <- rowSums(my_matrix)            # return vector of sum of all rows
            col_sum <- colSums(my_matrix)            # return vector of sum of all columns

            # matrix subsetting
            my_matrix[]                              # return entire matrix
            my_matrix[,]                             # return entire matrix
            my_matrix[, 1:5]                         # return all rows and columns 1-5
            my_matrix[1:10, c("A", "B")]             # return rows 1-10 and columns A and B

            # matrix concatenation
            C <- rbind(A, B)  # put rows of A and B together. Number of columns must be equal!
            C <- cbind(A, B)  # put columns of A and B together. Number of rows must be equal!

        # factors

            # create factor
            temperature <- c("Medium", "High", "Medium", "Low")
            temperature <- factor(temperature_vector, order = TRUE, levels = c("Low", "Medium", "High"))

            # create levels
            gender <- factor(c("M", "F", "F", "M", "M"))
            levels(factor_survey_vector) <- c("Female", "Male")  # replace M and F by Male and Female.

        # dataframes

            # create dataframes
            my_df <- data.frame(A=vector_A, B=vector_B)

            # dataframe functions
            str(my_df)  # get structure of data
            dim(my_df)  # get dimensions of dataframe

            # dataframe subsetting - same as matrices
            my_matrix[, 1:5]                         # return all rows and columns 1-5
            my_matrix[1:10, c("A", "B")]             # return rows 1-10 and columns A and B
            my_df$colname                            # return a column. Works if there is no space in column name.

            subset(my_df, subset=condition)

            # sort dataframe
            ordered_indices <- order(my_df, decreasing = FALSE)   # return indices of sorted elements
            my_df <- my_df[ordered_indices]                       # sort dataframe

        # list

            # create list
            my_list <- list(v=my_vector, m=my_matrix, df=my_df)

            # subsetting list
            list$elementname
            list[[2]]         # select second element

        # conditionals

            # logic operators
            == != > < >= <=
            &
            |
            !

            # if statement
            if (){
                condition
            } else if() {
                condition
            } else {

            }

        # loops

            # for loop
            for (item in sequence){
                # standard for loop to iterate over each item of a sequence
                break  # break for loop
                next   # skip further code and go to next iteration
            }

            for (i in 1:length(sequence) {
                # to get control over indices
            }

            # while loop
            while(condition is true){

            }

        # functions

            # named functions
            my_function <- function(a, b, c=1){
                # function body
                return(item)
            }

            # anonymous functions - mostly used as a parameter to other functions such as apply functions
            function(parameters){body}

        # apply functions
        apply(vector/matrix/df, 1/2, function, ...) # specify row/column. Mention function name without (). Pass parameters to function as additional arguments to apply() in place of ...
        lapply(list/vector/df, function, ...) # apply function to the vector. return a list.
        sapply(list/vector/df, function, ...) # apply functon on vector. if possible, return vector else matrix else list.
        tapply(function.applied.to.this.var, result.displayed.according.to.this.var, function)
        mapply()
        rapply()

        # helpful functions
        unqiue(vector)              # return unique elements of the vector
        sort(vector)                # return indices of sorted elements
        mean(vector)
        median(vector)
        var(vector)
        sd(vector)
        sum(vector)
        length(vector)
        abs(vector)
        round(vector)
        sort(vector)
        rep(x, times)
        seq(start, end, by=)       # return sequence from start to end. Use "by" to set difference between two numbers
        rev(vector)                # return reversed vector
        unlist(list)               # flatten a list
        append(v1, v2)             # return a list by appending two vectors one after the other
        is.*(object)               # check class of object by using "numeric", "character", "logical", "factor" or "vector" in place of *
        as.*(object)               # convert class of object by using "numeric", "character", "logical", "factor" or "vector" in place of *


        which.max(vector)
        which.min(vector)
        which(vector == "value")
        match("value",var)

        rnorm(n, mean = 0, sd = 1)
        sample(vector/number, size, replace = FALSE, prob = NULL) # trainSmall <- train[sample(nrow(train), x), ]

==============================================================================================================================

### dplyr

        library("dplyr")

        select(data_frame, column_names)                   # select a subset of columns
        filter(data_frame, condition1, condition2)         # subset dataframe based on conditions
        arrange(data_frame, desc(factor_variable))         # sort dataframe. Replace "desc(variable)" with "variable" for ascending order
        mutate(data_frame, variable_name = equation)       # create new or overwrite existing variable with or without using other variables
        group_by(data_frame, categorical_variable)         # group data by converting each category into one row.
        summarize(data_frame, new_var_name =  function())  # summarize vector into a number using functions such as mean() and sum(). group_by(categorical_variable) %>% summarize(count = n()) commonly used.
        count(data_frame, variable)                        # works similar table
        top_n(20, data_frame)                              # select top 20 rows

        piping : data_frame %>% operation1 %>% operation2 and so on...

==============================================================================================================================

### tidyr

        library(tidyr)

        separate(data_frame, variable.to.separate, c("new","names"), sep = ", ") # separate rohtak, haryana. Quotations important.

        unite(data_frame, new.variable, c(var1, var2), sep = " ") # combine first name and last name into name. No quotations for new variable names.

        gather(data_frame, new.factor.var, new.numerical.var, column.start.name:column.end.name) %>%
                filter(new.numerical.var == 1) %>%
                select(-new.numerical.var) # wide to long

        spread(data_frame, categorical.var, corresponding.numerical.var) # long to wide

==============================================================================================================================

## stringr

        library(stringr)

        tolower(character.variable)
        toupper(character.variable)

        substr("Amandeep", 1, 4)                     # return 'Aman'

        # check string presence
        grepl(pattern, character.variable)           # return logical vector whether pattern appears or not
        grep(pattern, character.variable)            # return indices of match in the variable
        str_detect(character.variable, pattern)      # check result

        # replace
        sub(pattern, replacement, vector)            # substitute only the first occurence of pattern
        gsub(pattern, replacement, vector)           # substitute all occurences of patterns
        str_replace(character.variable, "#", "%")    # same as gsub()

        # split
        strsplit(character.variable, pattern)        # split at given pattern

        # extract
        str_extract(variable, pattern)

        # miscellaneous
        str_length() or nchar()
        str_trim(character.vector)
        str_pad(character.vector, width = , side = "", pad = "")

==============================================================================================================================

### Linear Regression

	# model

		linear.model <- lm(target ~ ., data = train)
		linear.predictions <- predict(linear.model, cv)

    # model assessment

        # normality check
        qqnorm(variable)          # check whether all X and y that are used in modelling have normal distribution
        
        # check linearity between X and y, and independence, normality and homoscedasticity of residuals
        plot(residuals(model))    # plot should look like a perfect random noise
        abline(0,0)

    	# outlier check

	# model evaluation

        library(Metrics)
		error <- rmse(cv$target, linear.predict)

	# variable selection

		# interpret coefficients:
			# intercept: mean value of y when all numeric predictors are zero (or average value in case if they are centered) and all categorical predictors are set to default category
			# numeric predictors: unit increase in x increases y by coefficient units
			# categorical predictors: default category is absorbed in the intercept. y changes by coefficient unit if you use that particular level instead of default category
		# look at confidence intervals of coefficents
		# look at f-score of the model
		# remove variables one by one using p-value and vif
		# use anova test to see two models are same

		# tools for variable selection
		
			# model summary
			summary(linear.model)

			# CI for beta values
			confint.default(linear.model)  # asymptotic
			confint(linear.model)          # based on the profile-likelihood

			# test whether two models are the same
			anova(model.1, model.2)

			# automatic variable selection
			step.model <- step(linear.model, direction = c("both"))

			# vif: tells whether a predictor is a linear combination of another predictor
			vif(linear.model)              # vif <=5 is ok, 5 < vif < 10 is moderately correlated, vif > 10 is highly correlated

==============================================================================================================================

### Logistic Regression

	# binary classification

		glm.model <- glm(target ~ ., data = train, family = binomial(link=logit), control = list(maxit = 50))  # OR family = multinomial()
		glm.predictions <- predict(glm.model, cv, type = "response")

	# model assessment

		# normality check
		qqnorm(variable)          # check whether all X that are used in modelling have normal distribution

		# residual check: homoscedasticity and normality of residuals is not an assumption for logistic regression
		library(arm)
		binnedplot(y=glm.predictions, x=residuals(glm.model, "resp"),
				   xlab="Predicted probabilities", col.int="red4",
				   ylab="Average residuals",
				   main="Binned residual plot", col.pts="navy")
		# binnedplot:
			# 1. Order observations by predicted probabilities
			# 2. Create bins with equal observations. default: square root of number of observations
			# 3. Computer average residual in each bin
			# 4. Plot average residual vs predicted probabilities in each bin
			# note: can be used to check whether there is inverse logit association among the y and X: binnedplot(y=target, x=predictor)

	# model evaluation

		library(ROCR)
		library(pROC)
		library(e1071)
		library(Metrics)

		# confusion matrix
		confusionMatrix(ifelse(predictions >= 0.5, 1, 0), positive = "1")
		
		# classification error
		error <- ce(cv$target, glm.predict)
				
		# roc
		roc_curve <- roc(cv$target, algo.predict, plot=TRUE, auc=TRUE, grid=TRUE, col="blue")
		plot(roc_curve)
		lines(roc_curve2)  # add roc_curve2 to roc_curve plot

		# auc
		predict <- predict(model, cv)
		rocr <- prediction(predict, cv$target)
		auc <- as.numeric(performance(rocr, "auc")@y.values)

		# roc curve
		rocr <- prediction(abs(predict), cv$target)
		roc <- performance(rocr, "tpr", "fpr")
		plot(roc)

		# lift chart
		rocr <- prediction(abs(predict), cv$target)
		lift.chart <- performance(rocr, "lift", "rpp")  # google ROCR performance functions for more charts
		plot(lift.chart)

	# variable selection

		# all the linear regression techniques are applicable here except f-score test. A few things that are different from linear regression:
		# coefficients tell us about log-odds. 
			# numeric predictors: unit increase in x increases log-odds by coefficient
			# categorical predictors: change in log-odds by coefficient units had you used that particular category instead of the default category
		# exponents of coefficients tell us about odds. 
			# numeric predictors: unit increase in x multiplies odds by exp(coefficient)
			# categorical predictors: odds multiplied by exp(coefficient) if you use that particular category instead of the default category

		# f-score test becomes change in deviance test
		anova(model.1, model.2, test= "Chisq")

==============================================================================================================================

### K-Nearest Neighbor

		library(class)

		predictors <- names(train)[names(train) != "target"]
		knn.trainX <- train[, predictors]
		knn.cvX <- cv[, predictors]
		knn.trainY <- train$target
		knn.cvY <- cv$target

		# make sure all variables are numeric
		set.seed(1)
		knn.model <- knn(train = knn.trainX, test = knn.cvX, cl = knn.trainY, k = under.root.observations)
		confusionMatrix(knn.model, knn.cvY)

==============================================================================================================================

### Regularized Regression

		library("glmnet")

		# input numerical data only
		predictors <- names(train)[names(train) != "target"]
		trainX <- data.matrix(train[ ,predictors])
		trainY <- train$target
		cvX <- data.matrix(cv[ ,predictors])
		cvY <- cv$target

		cv.glmnet.model <- cv.glmnet(trainX, trainY,
									 type.measure = "deviance/mse/mae/class/auc",
									 family = "binomial",  # remove if not a classification problem
									 nfolds = 10, nlambda = 100)
		cv.glmnet.model
		plot(cv.glmnet.model)

		glmnet.model <- glmnet(trainX, trainY,
								family = "gaussian","binomial","multinomial",
								alpha = ,  # 0 - ridge, 1 - lasso, 0.5 - elnet
								lambda = cv.glmnet.model$lambda.1se  # or cv.glmnet.model$lambda.min
								)

		glmnet.predict <- predict(glmnet.model, cvX, s = cv.glmnet.model$lambda.1se)  # or cv.glmnet.model$lambda.min
		table(cv$target, glmnet.predict)

==============================================================================================================================

### SVM

		library("e1071")

		svm.model <- svm(target ~ ., data = train, kernel = "radial", cost = 1, gamma = 0.1)
		svm.predict <- predict(svm.model, cv)
		plot(svm.model, train)
		table(cv$target, svm.predict)

==============================================================================================================================

### Decision Trees

	# for classification use method = "class" in rpart and type = "class" in predict
		library("rpart")
		library("rpart.plot")

	# model

		tree.model <- rpart(target ~ ., data = train, method = "class/anova", parms = list(split = "gini/information"),
							control = rpart.control(cp = , minsplit = , minbucket = ))
		prp(tree.model)
		tree.predict <- predict(tree.model, cv, type = "class") # no need to specify "class" for regression
		table(cv$target, tree.predict)

	# ROCR and AUC(area under curve)

		tree.predict <- predict(tree.model, cv) # type = "class" not to be included here
		tree.ROCR <- prediction(tree.predict[,2], cv$target)
		tree.auc <- as.numeric(performance(tree.ROCR,"auc")@y.values)

==============================================================================================================================

### Random Forest

		library("randomForest")

	# model

		set.seed(10)
		rf.model <- randomForest(target ~ ., data = train,
								 importance = TRUE,
								 ntree = 1000,
								 mtry = root.of.variables,
								 do.trace = TRUE,
								 na.action = na.omit
								)
		varImpPlot(rf.model)
		rf.predict <- predict(rf.model, cv)
		table(cv$target, rf.predict)

	# party-cforest

		library("party")

		cforest.model = cforest(target ~ ., data = train, controls = cforest_unbiased(ntree = 1000, mtry = root.of.variables))
		cforest.prediction = predict(cforest.model, cv, OOB = TRUE, type = "response")
		table(cv$target, cforest.prediction)

==============================================================================================================================

### GBM

		library("gbm")

		gbm.model <- gbm(target ~ ., # if target doesn't work set to as.integer(target) after converting to numeric manually.
						 distribution = c("bernoulli","multinomial","gaussian"), # multinomial more robust even in binomial case
						 data = train,
						 n.trees = 2000,
						 interaction.depth = 1,
						 n.minobsinnode = 10,
						 shrinkage = 0.001,
						 train.fraction = 1.0,
						 keep.data = TRUE,
						 verbose = TRUE
						 )

		gbm.perf(gbm.model)

		gbm.predict <- predict(gbm.model, cv, n.trees = gbm.perf(gbm.model, plot.it = F), type = "response")
		gbm.predict <- apply(gbm.predict, 1, which.max) # choose class with maximum probability

		table(gbm.predict, cv$target)

==============================================================================================================================

### H2O deep-learning

		library(h2o)

		localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '2g')

		names(train) <- NULL # optional. so that the algorithm does not take column name as a separate level in the target variable
		h2o.train <- as.h2o(train)
		h2o.cv <- as.h2o(cv)

		h2o.model <- h2o.deeplearning(x = setdiff(names(train), c("target","id")), # predictors names or indices
							y = "target", # label. check names(h2o.train) to see target variable name
							training_frame = h2o.train, # data to train
							activation = "TanhWithDropout", # or 'Tanh'
							standardize = TRUE,
							input_dropout_ratio = 0.2, # % of inputs dropout
							hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
							balance_classes = TRUE, # for classification tasks only
							hidden = c(50,50,50), # two hidden layers with 100 nodes each
							epochs = 100, # number of iterations on data
							seed = 1, # for reproducability
							variable_importances = T,
							nfolds = 5)

		h2o.predictions <- as.data.frame(h2o.predict(h2o.model, h2o.cv))
		confusionMatrix(h2o.predictions$predict, cv$target) # remove first row of h2o.predictions if length does not match
		h2o.varimp_plot(h2o.model,num_of_features = 20)
		h2o.performance(h2o.model,xval = T)

==============================================================================================================================

### xgboost

		library(xgboost)

		# XGBOOST caret

		xgb.grid <- expand.grid(
		    eta = c(0.3,0.2,0.1,0.01),  # learning rate. [default=0.3][range: (0,1)]
			max_depth = c(6, 10),       # depth of tree. [default=6][range: (0,Inf)]
			nrounds = 100,              # iterations. [default=100]
			colsample_bytree = 1,       # proportion of features supplied to a tree. [default=1][range: (0,1)]
		    subsample = 1,              # proportion of samples supplied to a tree. [default=1][range: (0,1)]
		    min_child_weight = 1,       # minimum number of instances required in a child node. [default=1][range:(0,Inf)]
		    gamma = c(0,5)              # increased gamma to increase level of regularization. [default=0][range: (0,Inf)]
		    lambda = 0,                 # L2 regularization(Ridge). [default=0]
		    alpha = 1                   # L1 regularization(Lasso). more useful on high dimensional data sets. [default=1]
		    )

		xgb.control <- trainControl(
		    method="cv",
		    number = 5,
		    verboseIter = TRUE,
		    returnData = FALSE,
		    returnResamp = "all",
		    allowParallel = TRUE
			)

		xgb.predictors <- as.matrix(train[, !(names(train) %in% c("target","id"))])
		xgb.label <- train$target

		xgb.model <- train(x = xgb.predictors,
		    y = xgb.label,
		    trControl = xgb.control,
		    tuneGrid = xgb.grid,
		    method="xgbTree" # "xgbLinear"
		)

		xgb.predict <- predict(xgb.model, data.matrix(cv))
		confusionMatrix(xgb.predict, cv$target)
		important.features <- varImp(xgb.model)
		plot(important.features, 20)

		# XGBOOST manual

		data.train <- xgb.DMatrix(data = train$data, label = train$label)
		data.cv <- xgb.DMatrix(data = cv$data, label = cv$label)

		watchlist <- list(train  = data.train, test = data.cv)

		parameters <- list(
      # General Parameters
          booster            = "gbtree",          # default = "gbtree"
          silent             = 0,                 # default = 0
      # Booster Parameters
          eta                = 0.3,               # default = 0.3, range: [0,1]
          gamma              = 0,                 # default = 0,   range: [0,∞]
          max_depth          = 6,                 # default = 6,   range: [1,∞]
          min_child_weight   = 1,                 # default = 1,   range: [0,∞]
          subsample          = 1,                 # default = 1,   range: (0,1]
          colsample_bytree   = 1,                 # default = 1,   range: (0,1]
          colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
          lambda             = 1,                 # default = 1
          alpha              = 0,                 # default = 0
      # Task Parameters
          objective          = "reg:logistic",    # default = "reg:linear"
          eval_metric        = "error",
          num_classes        = 2,                 # number of classes in case of multi class classification
          seed               = 1234				  # reproducability seed
        )

		xgb.model <- xgb.train(parameters, data.train, nrounds = 20, watchlist)
		xgb.predict <- predict(xgb.model, data.cv)
		confusionMatrix(xgb.predict, cv$target)

		predictor.names <- dimnames(data.matrix(train[, !(names(train) %in% c("target"))]))[[2]]
		importance_matrix <- xgb.importance(predictor.names, model = xgb.model)
		xgb.plot.importance(importance_matrix[1:20,])  # plot top 20 important variables

==============================================================================================================================

### caret

		library("caret")

		f <- as.formula(paste("target ~", paste(names(train)[!names(train) %in% c("target")], collapse = " + ")))

		algo.control = trainControl(method = "cv",
									number = 5,
									classProbs = T,
									summaryFunction = defaultSummary,  # twoClassSummary for binary classification
									verboseIter = T,
									allowParallel = T
									)

		algo.grid = expand.grid(model_specific_parameters)

		algo.model <- train(target ~ .,
							data = train,
							method = "",
							preProcess = c("center","scale"),  # zv/nzv, medianImpute/knnImpute, center, scale, pca - imp for linear models
							metric = "",
							trControl = algo.control,
							tuneGrid = algo.grid
							)

		trellis.par.set(caretTheme())
		plot(algo.model, metric = "Accuracy")  # look at ?plot.train

		algo.predict <- predict.train(algo.model, cv, type = "raw/prob")
		confusionMatrix(algo.predict, cv$target, mode = "prec_recall", positive = "class_name")

		imp <- varImp(algo.model)
		plot(imp, top = 20)

		# compare models
		model_list <- list(RF = rf.model,
						   GBM = gbm.model,
						   )
		resamps <- resamples(model_list)
		resamps
		summary(resamps)

		bwplot(resamps, layout = c(3, 1))
		dotplot(resamps, metric = "ROC")
		densityplot(resamps, metric = "ROC")
		xyplot(resamps, metric = "ROC")

==============================================================================================================================

### K-fold cross validation

		# Set number of folds
		k <- 10

		# Randomly shuffle the data
		data.frame <- data.frame[sample(nrow(data.frame)), ]

		# Create K equally size folds
		folds <- cut(seq(1, nrow(data.frame)), breaks = k, labels = FALSE)

		accuracy <- vector(mode = "integer", length = k)
		x <- 0

		# Perform K-fold cross validation
		for(i in 1:k){
		    if(i == 1) cat("Fold", "\t", "Accuracy", "\n")

		    x <- x + 1
		    #Segment your data by fold using the which() function
		    cv.indices <- which(folds == i, arr.ind=TRUE)
			train <- data.frame[-cv.indices, ]
			cv <- data.frame[cv.indices, ]

			# model

			# calculate accuracy for current fold
			accurate.predictions <- 0
			confusion.matrix <- as.matrix(table(cv$target, predictions))

			for(i in 1:nrow(confusion.matrix)){
				accurate.predictions <- accurate.predictions + confusion.matrix[i,i]
			}

			accuracy[x] <- accurate.predictions/nrow(cv)
			cat(x, "\t", round(accuracy[x], 4), "\n")
			if(x == k)
			{
				cat("Mean Accuracy", "\n")
				cat(round(mean(accuracy), 4))
			}
		}

==============================================================================================================================
### PCA

		pca <- prcomp(data_frame, center = TRUE, scale = TRUE)
		summary(pca)
		plot(pca, type="l")
		pca.data <- pca$x

==============================================================================================================================
### PCR

		library(pls)
		pcr.model <- pcr(target ~ ., data = train, scale =T, center = T, validation = "CV")
		pcr.predict <- predict(pcr.model, cv)
		rmse(pcr.predict, cv$target)

==============================================================================================================================
### Clustering

	data_frame -> as.matrix -> as.vector -> clustering -> dim(vector) -> image.output
	data_frame -> as.matrix -> as.vector -> test

	# after clustering we can subset the data according to the clusters(for hierarchical especially) to study number of rows in each cluster.
	# cluster1 <- subset(train , cluster == nth.cluster.number)

	# Hierarchical

		distances <- dist(movie[2:20] OR vector, method = "euclidean")
		hcluster <- hclust(distances, method = "ward.D")
		plot(hcluster)
		hclusterGroups <- cutree(cluster, k = no.of.clusters)
		hclusterGroups[index.of.var.to.see.which.cluster.it.belongs.to]

	# K-Means

		library("flesclust")
		set.seed(1)
		kcluster <- kmeans(vector OR data_frame, centers = no.of.centroids, iter.max = no.of.max.iterations)
		str(kcluster)

		# rest of this for cluster-then-predict
		# in cluster-then-predict problems, (remove target var->normalize(optional)->cluster->kcca)=>build "k" train and test sets using subset from original train according to clusters.
		# example -> newTrain1 = subset(originalTrain, kclusterPredictTrain == 1), newTrain2 = subset(originalTrain, kclusterPredictTest == 2)

		kclusterKcca <- as.kcca(kcluster, originalDataFrame OR originalVector)
		kcluster.predict.train <- predict(kclusterKcca)
		kcluster.predict.test <- predict(kclusterKcca, newdata = test.data.as.vector)

		#easy way to split according to clusters in k-means

		KmeansCluster = split(data_frame, kcluster$cluster) # KmeansCluster[[1]] and so on to access 1st and other successive clusters

==============================================================================================================================

### Ensembling

	# 1. Split data into train, cv and test. Create multiple models using different ml algorithms on train set.
	# 2. Make a data frame ensemble_train which includes predictions of these multiple models (on cv) in each column along with the cv target variable.
	# ex: suppose model1 and model2 predict pred1_cv and pred2_cv on cv set then this ensemble_train contains pred1_cv,pred2_cv,cv_targetVariable.

	# 3. Now make predictions on test set using each of these models, say pred_test1, pred_test2 and make a data frame called ensemble_test where these are the columns.
	# 4. Now make a model ensemble_model which is built using ensemble_train and predicts on ensemble_test. This is the final prediction.

	predictions <- data.frame(algo1.prediction = , algo2.prediction = , algo3.prediction = ,
								final.prediction = rep(0, nrow(cv)), actual.label = cv$target)

    getmode <- function(x) {
    unique.x <- unique(x)
    unique.x[which.max(tabulate(match(x, unique.x)))]
	}

	predictions$final.prediction <- apply(predictions, 1, getmode)

==============================================================================================================================

### Splitting data set randomly

	# sample.split balances partitions keeping in mind the outcome variable

		library("caTools")
		set.seed(10)
		split <- sample.split(data_frame$target, SplitRatio = 0.8)
		train <- subset(data_frame, split == TRUE)
		cv <- subset(data_frame, split == FALSE)

	# using sample

		set.seed(123)
		indices <- sample(2, nrow(data_frame), replace = T, prob = c(0.75, 0.25))
		train <- data_frame[indices == 1, ]
		cv <- data_frame[indices == 2, ]

	# caret

		train_indices <- createDataPartition(train$target, p = 0.75, list = FALSE)
		train <- Sonar[train_indices, ]
		cv  <- Sonar[-train_indices, ]

==============================================================================================================================

### Sampling Techniques

	# sampling is to be done in case of highly unbalanced class. Only training data has to be sampled.

		# ROSE

			library(ROSE)

			# oversampling
			oversampled_train_data <- ovun.sample(target ~ ., data = train, method = "over",
	                                    	N = 2*length(train$target[train$target == "class_with_more_observations"]),
	                                      	seed = 1)$data

			# undersampling
			undersampled_train_data <- ovun.sample(target ~ ., data = train, method = "under",
	                                    	N = 2*length(train$target[train$target == "class_with_less_observations"]),
	                                      	seed = 1)$data

			# mixed sampling
			mix_sampled_train_data <- ovun.sample(target ~ ., data = train, method = "both", p=0.5,
	                                  		N=nrow(train),
	                                  		seed = 1)$data

			# ROSE (type of mixed sampling)
			rose_train_data <- ROSE(target ~ ., data = train, seed = 1)$data

		# SMOTE

			library(DMwR)
			smote_train_data <- SMOTE(target ~ ., data = train, perc.over = 100, perc.under = 200, k = 5)

			library(smotefamily)
			library(FNN)

			smote_train_data <- SMOTE(train[, !colnames(train) %in% c("target")], train$target, K = 5, dup_size = 0)

		# caret

			library(caret)

			set.seed(4)
			caret_train_date <- downSample(x = train[, !colnames(train) %in% c("target")], y = train$target)
			caret_train_date <- upSample(x = train[, !colnames(train) %in% c("target")], y = train$target)

			OR

			algo.control = trainControl(method = "cv",
										number = 5,
										classProbs = T,
										summaryFunction = defaultSummary,  # twoClassSummary for binary classification
										verboseIter = T,
										allowParallel = T,
										sampling = "up/down/smote/rose"
										)

==============================================================================================================================

### one-hot Encoding

		library(caret)

		dummies <- dummyVars(target ~ ., data = data_frame)
		temp <- as.data.frame(predict(dummies, data_frame))

==============================================================================================================================

### handling missing values

	# To be run only on variables having missing value. For convinience run on every variable except for dependent variable.

		# check missing values
		colSums(is.na(data_frame))*100/nrow(data_frame)
		sapply(data_frame, function(x) sum(is.na(x))*100/length(x))

		library("Hmisc")
		df_imputed <- aregImpute(~ x1 + x2 + x3, data = data_frame, n.impute = 5)
		df_imputed
		df_imputed$imputed$x1

		library("missForest")
		df_imputed <- missForest(data_frame)
		df_imputed$ximp

		library("mice")
		md.pattern(data_frame)

		set.seed(10)
		imputed <- complete(mice(data_frame))
		data_frame$var <- imputed$var

		library(DMwR)
		inputData <- knnImputation(inputData)

		knn.impute(data, k = 10, cat.var = 1:ncol(data), to.impute = 1:nrow(data), using = 1:nrow(data))

==============================================================================================================================

### Removing Variables

		data_frame <- data_frame[ , !(names(data_frame) %in% c("x1","x2","x3"))]
		data_frame$var <- NULL
		new_data_frame <- setdiff(names(data_frame), c("x1","x2","x3"))  # check if setdiff() or data_frame[, setdiff()]

==============================================================================================================================

### Correlation and scatterplot matrix

	# correlation matrix

		library(corrplot)

		numeric <- sapply(data_frame, is.numeric)
		cordata <- data_frame[ ,numeric]
		cordata <- na.omit(cordata)
		cor_matrix <- cor(cordata) # to see correlation table
		cor_matrix
		corrplot(cor_matrix, method = "square", type = "lower") # to visualize correlation matrix

	# scatterplot matrix

		pairs(cordata) # scatterplot matrix

==============================================================================================================================

### Binning

		library("Hmisc")

		bins <- cut2(data_frame$variable, g = number_of_bins) # cuts the variable on basis of quantile.

==============================================================================================================================

### Check constant factors

		logical <- sapply(data_frame, function(x) is.factor(x))
		factor.variables <- data_frame[ , names(which(logical == TRUE))]
		ifelse(factor.variables <- sapply(factor.variables, function(x) length(levels(x))) == 1,"constant_factor","0")
		table(factor.variables)

==============================================================================================================================

### Boruta Analysis to find importance of variable in a data set

	library(Boruta)

		set.seed(13)
		boruta.model <- Boruta(targetVariable ~ ., data = train, maxRuns=101, doTrace=0)
		summary(boruta.model)
		boruta.cor.matrix <- attStats(boruta.model)
		important.features <- names(data_frame)[boruta.model$finalDecision!="Rejected"]
		important.features

==============================================================================================================================

## text-analysis

		library(tm)
		library(SnowballC)
		library(wordcloud)
		library(RColorBrewer)

	# cleaning data

		bag <- Corpus(VectorSource(data_frame$textVariable))

		bag <- tm_map(bag, tolower)

		bag <- tm_map(bag, PlainTextDocument)

		bag <- tm_map(bag, removePunctuation)

		bag <- tm_map(bag, removeWords, c("your custom words", stopwords("english")))

		bag <- tm_map(bag, stripWhitespace)

		bag <- tm_map(bag, stemDocument)

	# converting to data frame

		frequencies <- DocumentTermMatrix(bag)

		findFreqTerms(frequencies, lowfreq = 20) # to words that appear atleast 20 times

		sparseWords <- removeSparseTerms(frequencies, 0.995) # words that appear in 99.5 percent tweets

		sparseWords <- as.data.frame(as.matrix(sparseWords)) # converting the matrix of sparse words to data frame

		colnames(sparseWords) <- make.names(colnames(sparseWords))

	# wordcloud

		bag <- TermDocumentMatrix(bag)
		bag <- as.matrix(bag)
		bag <- sort(rowSums(bag), decreasing = T)
		bag.df <- data.frame(word = names(bag), freq = bag)

		set.seed(154)

		wordcloud(words = bag.df$word, freq = bag.df$freq, min.freq = 30,
	    			max.words=1500, random.order=FALSE, rot.per=0.35,
        			colors=brewer.pal(8, "Dark2"), scale = c(5,0.5))

==============================================================================================================================

## Data Table

		library(DT)
		datatable(data_frame)

		# table in R markdown
		kable(data_frame)

==============================================================================================================================

### Date and Time

        # get date and time
        today <- Sys.Date()
        now <- Sys.time()

        # manipulate date and time
		data_frame$date <- as.POSIXct(strptime(variable, format = ""))  # format is like "%d/%m/%Y %H:%M:%S"
		data_frame$day <- as.integer(format(data_frame$date, "%d"))     # day
		data_frame$month <- as.factor(format(data_frame$date, "%B"))    # month
		data_frame$year <- as.integer(format(data_frame$date, "%Y"))    # year
		data_frame$weekday <- as.factor(format(data_frame$date, "%A"))  # weekday
		data_frame$hour <- as.integer(format(data_frame$date, "%H"))    # hour
		data_frame$minute <- as.integer(format(data_frame$date, "%M"))  # minute
        data_frame$time <- format(time, "%I:%M %p")                     # extract time in the form of 11:26 AM
        # to get full list of date formats check - https://www.statmethods.net/input/dates.html

==============================================================================================================================

### Efficient Code

	# Never grow a vector. Always use matrices instead of data frames whenever possible.

		# check time taken by code
		system.time(
			# code
			)

	library(microbenchmark)

		# evaluate and compare time taken by multiple expressions expressions
		microbenchmark(rnorm(1000), 1:1000, times = )  # times to run each expression

	library(benchmarkme)

		# time taken to read and write csv files
		benchmark_io(runs = , size = )  # size in MBs

	library(profvis)

		# show memory used and CPU time taken by code
		profvis({
			# code
			})

	library(parallel)

		# Print no_of_cores
		detectCores()

		# assign cores to use
		num_cores <- detectCores()

		# start cluster
		cluster <- makeCluster(num_cores)

		# optional: makeCluster does not include functions present in the envinronment.
		clusterExport(cluster, "function name as string")  # only pass function name within quotations.

		# code
		parApply(cluster, array/matrix/data_frame, 1/2, function, ...)
		parSapply(cluster, list/vector/data_frame, function, ...)
		parLapply(cluster, list/vector/data_frame, function, ...)

		# close cluster
		stopCluster(cluster)

==============================================================================================================================
