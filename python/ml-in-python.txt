
#### scikit-learn

	### supervised learning

		## get X and y

			# make artificial dataset
			from sklearn.datasets import make_classification
			X, y = make_classification(n_samples = , n_features = ,
									   n_informative = , n_redundant = ,
									   n_repeated = , random_state = ,
									   weights = [0.5, 0.5]
				   )

			# divide data into X and y
			X = df.drop("target_column", axis = 1).values
			y = df.target_column.values
			
			# encode class values as integers
			from sklearn.preprocessing import LabelEncoder
			encoder = LabelEncoder()
			encoder.fit(y)
			encoder.classes_
			encoded_y = encoder.transform(y)
			y = encoder.inverse_transform(encoded_y)

		## preprocess: sklearn-pandas pipeline

			# divide dataframe into predictors and target
			X = df.drop("target_column", axis = 1)
			y = df.target_column

			# drop na observations if possible

			# 1. basic preprocessing - impute and scale numeric features, impute and encode categorical features
			from sklearn_pandas import DataFrameMapper
			from sklearn_pandas import CategoricalImputer
			preprocess = DataFrameMapper([
	        	(['num_var_1'], [Imputer(), StandardScaler()]),
	        	(['num_var_2'], [Imputer(), StandardScaler()]),
	        	(['cat_var_1'], [CategoricalImputer(), LabelEncoder()]),
	        	(['cat_var_2'], [CategoricalImputer(), LabelEncoder()])],
	        df_out=True)
			
			preprocess.fit(X)
			X = preprocess.transform(X)
			X = X.apply(lambda column: column.astype(np.float64))
			
			# 2. advance preprocessing - one hot encode some of the categorical variables
			one_hot_encode = DataFrameMapper([
	        	(['num_var_1'], None),
	        	(['num_var_2'], None),
	        	(['cat_var_1'], OneHotEncoder()),                # one hot encode variables with less categories
	        	(['cat_var_2'], None)],                          # don't one hot encode variables with a lot of categories
	    	df_out=True)

			one_hot_encode.fit(X)
			X = one_hot_encode.transform(X)

			# preprocess y
			from sklearn.preprocessing import LabelEncoder
			encoder = LabelEncoder()
			encoder.fit(y)
			y = encoder.transform(y)

			# when you get the test data in the future, just transform it using the basic and advance processing steps
			test_predictors = preprocess.transform(test_predictors)
			test_predictors = test_predictors.apply(lambda column: column.astype(np.float64))
			test_predictors = one_hot_encode.transform(test_predictors)
			test_target = encoder.fit(test_target)

		## preprocess: sklearn pipeline

			from sklearn.pipeline import Pipeline
			from sklearn.pipeline import FeatureUnion
			from sklearn.base import BaseEstimator, TransformerMixin
			from sklearn.preprocessing import OneHotEncoder
			from category_encoders import OrdinalEncoder

			class DataFrameSelector(BaseEstimator, TransformerMixin):
				def __init__(self, attribute_names):
					self.attribute_names = attribute_names
				def fit(self, X, y=None):
					return self
				def transform(self, X):
					return X[self.attribute_names]

			# divide dataframe into predictors and target
			X = df.drop("target_column", axis = 1)
			y = df.target_column

			# preprocess X
			num_attributes = []
			cat_attributes = []

			num_pipeline = Pipeline([
							('selector', DataFrameSelector(num_attributes)),
							('imputer', Imputer()),
							('scaler', StandardScaler())
						])
 
			cat_pipeline = Pipeline([
							('selector', DataFrameSelector(cat_attributes)),
							('imputer', CustomImputer(strategy='mode')),
							('label_encoder', OrdinalEncoder()),
							('one_hot_encoder', OneHotEncoder())  # avoid this step if too much categories in a column
						])

			full_pipeline = FeatureUnion(transformer_list=[
								("num_pipeline", num_pipeline),
								("cat_pipeline", cat_pipeline)
						])

			full_pipeline.fit(X)
			X = full_pipeline.transform(X)
			
			# preprocess y
			from sklearn.preprocessing import LabelEncoder
			encoder = LabelEncoder()
			encoder.fit(y)
			y = encoder.transform(y)

			# when you get the test data in the future, just transform it using the pipeline
			test_predictors = full_pipeline.transform(test)
			test_target = encoder.fit(test_target)

			# split data into train and test
			from sklearn.model_selection import train_test_split
			X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)
			
	### models

		## linear regression

			from sklearn.linear_model import LinearRegression
			model = LinearRegression()                         # instantiate model
			model.fit(X_train, y_train)                        # fit model
			y_pred = model.predict(X_test)                     # predict using model
			model.score(X_test, y_test)                        # evaluate model


		## regularization

			# lasso - l1
			from sklearn.linear_model import Lasso             # l1-regularization
			model = Lasso(alpha = , normalize = )
			model.fit(X_train, y_train)
			coefficients = model.fit(X_train, y_train).coef_   # extract coefficients for each feature
			plt.plot(range(len(names)), coefficients)          # variable importance
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)

			# ridge - l2
			from sklearn.linear_model import Ridge             # l2-regularization
			model = Ridge(alpha = , normalize = )
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)

			# elasticnet - l1 + l2
			from sklearn.linear_model import ElasticNet        # mix of l1 and l2 regularization
			model = ElasticNet(alpha = , normalize = )
			model.fit(X_train, y_train)
			coefficients = model.fit(X_train, y_train).coef_   # extract coefficients for each feature
			plt.plot(range(len(names)), coefficients)          # variable importance
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)


		## logistic regression
		
			from sklearn.linear_model import LogisticRegression
			from sklearn.feature_selection import RFE
			model = LogisticRegression(penalty  =  "l2",
									   C        =  1.0,
									   verbose  =  1
			)
			# rfe = RFE(model, n)                              # recursive feature elimination. select top 'n' features
			# fit = rfe.fit(X_train, y_train)
			# print(fit.n_features_, fit.support_, fit.ranking_)
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)

			# multiclass logistic regression
			from sklearn.multiclass import OneVsRestClassifier
			model = OneVsRestClassifier(LogisticRegression())  # also used when the target column is changed to dummy target columns


		## knn

			from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
			model = KNeighborsClassifier(n_neighbors = 6)
			model = KNeighborsRegressor()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)


		## support vector machine

			from sklearn.svm import SVC, SVR
			model = SVC(C = , kernel = "")
			model = SVR(C = , kernel = "")
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)


		## decision tree

			from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
			import graphviz
			model = DecisionTreeClassifier(criterion         =  "gini/entropy",
										   max_depth         =  ,
										   min_samples_split =  ,
										   min_samples_leaf  =  ,
										   max_features      =  ,
										   max_leaf_nodes    =  ,
										   min_impurity_decrease = ,
										   class_weight      =  "balanced",
										   presort           =  "True"        # faster training when dataset is small
					)
			model = DecisionTreeRegressor()  # same parameters as DecisionTreeClassifier()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)
			
			# feature importance
			model.feature_importances_
			
			# plot tree
			model.tree_
			graph_data = export_graphviz(model, 
										 out_file            =   None,
										 feature_names       =   ,
										 class_names         =   ,
										 filled              =   True,
										 rounded             =   True,  
										 special_characters  =   True
						 )
			graph = graphviz.Source(graph_data) 
			graph.render("tree")  # save tree as tree.pdf file


		## random forest

			from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
			model = RandomForestClassifier(n_estimators        = ,            # number of trees
										   max_features        = ,            # number of features at each node split
										   oob_score           = True,
										   random_state        = 4,           # random seed
										   verbose             = 1

					)
			model = RandomForestRegressor()          # same parameters as RandomForestClassifier()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)
			
			# feature importance
			model.feature_importances_

			# oobs score
			model.oob_score_


		## ada boost

			from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
			model = AdaBoostClassifier(base_estimator    = ,                  # optional
									   n_estimators      = 100,               # number of trees
									   learning_rate     = 0.1,               # trade-off between learning_rate and n_estimators
									   random_state      = 4,                 # random seed

					)
			model = AdaBoostRegressor()              # same parameters as AdaBoostClassifier()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)
			model.feature_importances_
			model.estimator_errors_
			model.estimator_weights_


		## gradient boosting

			# start with a small learning rate such as 0.01 - find optimal number of trees using training and validation error plot
			# most important parameters to tune are - learning_rate, n_estimators, max_depth, subsample and max_features
			from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
			model = GradientBoostingClassifier(loss                = "deviance",
											   learning_rate       = 0.1,         # good choice - [<0.1]
											   n_estimators        = 100,         # number of trees - good choice - [100-2000]
											   max_depth           = 3,           # depth of tree - good choice - [3-7]
											   max_leaf_nodes      = None,        # max_depth=k is equivalent to max_leaf_nodes=k+1 -- use one of them -- using max_leaf_nodes is significantly faster but gives slightly higher error
											   subsample           = 1.0,         # subset of rows - good choice - [0.3-0.7]
											   max_features        = None,        # subset of columns - good choice ['sqrt', 0.4-0.8]
											   min_samples_split   = 2,
											   min_samples_leaf    = 1,
											   verbose             = 1,
											   random_state        = 4,

					)
			model = GradientBoostingRegressor()             # same parameters as GradientBoostingClassifier()
			model.fit(X_train, y_train)
			y_pred = model.predict(X_test)
			model.score(X_test, y_test)
			model.feature_importances_
			model.train_score_[i]                           # train score of i-th iteration

		## xgboost

			# start with a small learning rate such as 0.01 - find optimal number of trees using training and validation error plot
			# most important parameters to tune are - learning_rate, n_estimators, max_depth, subsample and colsample
			
			# scikit-learn wrapper
			from xgboost import XGBClassifier
			model = XGBClassifier(learning_rate      = 0.1,     # good choice - [<0.1]
								  n_estimators       = 100,     # number of trees - good choice - [100-2000]
								  max_depth          = 3,       # maximum depth of tree - good choice - [3-7]
								  subsample          = 1.0,     # subset of rows - good choice - [0.3-0.7]
								  colsample_bytree   = 1.0,     # subset of columns at each tree - good choice ['sqrt', 0.4-0.8]
								  colsample_bylevel  = 1.0,     # subset of columns at each split - good choice ['sqrt', 0.4-0.8]
								  nthread            = -1       # number of cores to use
			)

			eval_set = [(X_train, y_train), (X_test, y_test)]
			eval_metric = 'rmse'/'mae'/'error'/'logloss'/'mlogloss'/'auc'   # pass a list to pass multiple metrics

			model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="error", eval_set=eval_set, verbose=True)
			y_pred = model.predict(X_test)
			predictions = [round(value) for value in y_pred]
			model.score(X_test, y_test)
			
			# feature importance
			model.feature_importances_
	        xgb.plot_importance(model)                          # import matplotlib for this
	        
	        # plot tree
	        xgb.plot_tree(model,                                # import graphviz for this
	        			  num_trees = 2,                        # plot 2nd tree
	        			  rankdir   = 'LR'                      # plot tree in a left-to-right format
	        )
	        
			# plot learning curve

				results = model.evals_result()
				epochs = len(results['validation_0']['error'])
				x_axis = range(0, epochs)
				
				# plot evaluation metric
				fig, ax = pyplot.subplots()
				ax.plot(x_axis, results['validation_0']['logloss'], label='Train')
				ax.plot(x_axis, results['validation_1']['logloss'], label='Test')
				ax.legend()
				pyplot.ylabel('Log Loss')
				pyplot.title('XGBoost Log Loss')
				pyplot.show()

			# manual xgboost
			import xgboost as xgb
			dtrain = xgb.DMatrix(X_train, label = y_train)
			dtest = xgb.DMatrix(X_test, label = y_test)
			parameters = dict(
				
				# general parameters
				booster            = "gbtree",          		# default = "gbtree"
				silent             = 0,                 		# default = 0

				# booster parameters
				eta                = 0.3,               		# default = 0.3, range: [0,1]
				gamma              = 0,                 		# default = 0,   range: [0,∞]
				max_depth          = 6,                 		# default = 6,   range: [1,∞]
				min_child_weight   = 1,                 		# default = 1,   range: [0,∞]
				subsample          = 1,                 		# default = 1,   range: (0,1]
				colsample_bytree   = 1,                 		# default = 1,   range: (0,1]
				colsample_bylevel  = 1,                 		# default = 1,   range: (0,1]
				lambda             = 1,                 		# default = 1
				alpha              = 0,                 		# default = 0

				# task parameters
				objective          = "binary:logistic", 		# default = "reg:linear"
				eval_metric        = ["error", "logloss"],
				num_classes        = 2,                 		# number of classes in case of multi class classification
				seed               = 4				  		    # reproducability seed
	        )

	        eval_list  = [(dtest,'eval'), (dtrain,'train')]
	        
	        model = xgb.train(parameters, dtrain, num_round = 10, eval_list, early_stopping_rounds = 10)
	        y_pred = model.predict(dtest, ntree_limit = model.best_ntree_limit)  # pass ntree_limit only if early_stopping_rounds used


        ## voting ensemble

	        from sklearn.ensemble import VotingClassifier
	        estimators = [
	        			("knn", KNeighborsClassifier()),
	        			('cart', DecisionTreeClassifier()),
	        			('svm', SVC())
	        ]
	        model = VotingClassifier(estimators, voting='soft', weights=[2,1,2], n_jobs=-1)
	        cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)

	    ## save/load model
	    	
	    	# the python version and library versions almost certainly need to be the same while loading a saved model

	        # save model to disk
	        from pickle import dump
	        dump(model, open("filename.sav", 'wb'))

	        from sklearn.externals.joblib import dump
	        dump(model, "filename.sav")

	        # load model from disk
	        from pickle import load
	        model = load(open("filename.sav", 'rb'))
	        
	        from sklearn.externals.joblib import load
	        model = load("filename.sav")

	### model evaluation

		## return default metric
			model.score(X_test, y_test)                     # return accuracy/r-squared
			
		## regression

			from sklearn.metrics import r2_score
			r2_score(y_test, y_pred)

			from sklearn.metrics import mean_squared_error
			mean_squared_error(y_test, y_pred)

		## classification

			from sklearn.metrics import accuracy_score
			accuracy_score(y_test, y_pred)

			from sklearn.metrics import confusion_matrix
			cm = confusion_matrix(y_test, y_pred)
			def print_accuracy(cm):
				tn = cm[0][0]
				fp = cm[0][1]
				fn = cm[1][0]
				tp = cm[1][1]
				sensitivity = tp/(tp+fn)
				specificity = tn/(tn+fp)
				accuracy = (tp+tn)/(tn+fp+fn+tp)
				print(pd.DataFrame({"Accuracy": [accuracy], "Sensitivity": [sensitivity], "Specificity": [specificity]}))

			from sklearn.metrics import classification_report
			classification_report(y_test, y_pred)

			from sklearn.metrics import roc_curve
			y_pred_prob = model.predict_proba(X_test)[:, 0/1]  # return probabilites of class 0/1
			fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

			from sklearn.metrics import roc_auc_score
			y_pred_prob = model.predict_proba(X_test)[:, 0/1]
			roc_auc_score(y_test, y_pred_prob)

			from sklearn.metrics import log_loss
			log_loss(y_test, y_pred)                           # see scikit-learn documentation

		## cross-validation

			# create folds
			from sklearn.model_selection import KFold
			from sklearn.model_selection import StratifiedKFold  # useful in case of imbalanced class
			from sklearn.model_selection import LeaveOneOut
			from sklearn.model_selection import ShuffleSplit     # multiple train/test sets
			from sklearn.model_selection import cross_val_score
			
			folds = KFold(n_splits = 5, shuffle = True, random_state = 4)
			folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)
			folds = LeaveOneOut()
			folds = ShuffleSplit(n_splits = 5, test_size = 0.3, random_state = 4)

			# cross-validation without hyperparameter tuning - select best model among different models
			score = "r2/neg_mean_squared_error/neg_mean_absolute_error/accuracy/neg_log_loss/roc_auc/f1/recall"
			cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score, n_jobs=-1, verbose=1)  # pass model/pipeline after instantiating it
			cv_results.mean()*100
			cv_results.std()*100

			# manual cross validation
			for train_index, test_index in folds.split(X_train, y_train):
				model.fit(X_train[train_index], y_train[train_index])
				y_pred = model.predict(X_train[test_index])
				model.score(X_train[test_index], y_train[test_index])

			# hyperparameter tuning - select best hyperparameters of a model among different hyperparameters
			from sklearn.model_selection import GridSearchCV
			from sklearn.model_selection import RandomizedSearchCV
			
			params = {"parameter_name": [parameter_values]}
			params = [
					{'param_1': [1, 2], 'param_2':, [4, 5]},   # try all combinations - (1, 4), (1, 5), (2, 4) and (2, 5)
					{'param_1': [1], 'param_2': [6, 7]}        # also try (1, 6) and (1, 7)  but not (2, 6) and (2, 7)
			]
			
			# tuning hyperparameters while using pipeline
			params = {'stepname__paramname': [parameter_values]}
			
			model_cv = GridSearchCV(estimator   = model,       # pass model/pipeline after instantiating it
									param_grid  = params,
									scoring     = score,
									cv          = folds,
									verbose     = 1            # the higher, the more messages
						)
			
			model_cv = RandomizedSearchCV(estimator             = model,
										  param_distributions   = params,
										  n_iter                = 100,
										  scoring               = score,
										  cv                    = folds,
										  verbose               = 1,
										  random_state          = 4
						)
			model_cv.fit(X_train, y_train)                     # search takes place here
			model_cv.cv_results_
			model_cv.best_score_
			model_cv.best_params_

		## feature selection

			# train model on complete data
			model.fit(X_train, y_train)

			# select threshold using feature importance
			thresholds = sort(model.feature_importances_)
			
			# build model using all the thresholds - builds model using one best feature, two best features and so on..
			for thresh in thresholds:
				
				# select a subset of features in the train and test data using threshold
				selection = SelectFromModel(model, threshold=thresh, prefit=True)
				X_train_subset = selection.transform(X_train)
				X_test_subset = selection.transform(X_test)

				# re-train model on subset X_train
				model.fit(X_train_subset, y_train)
				
				# evaluate model on subset X_test
				score = model.score(X_test_subset, y_test)

				print("Thresh=%.3f, n=%d, Score: %.2f%%" % (thresh, X_train_subset.shape[1], score*100.0))

		## compare machine learning models

			# prepare models
			models = []
			models.append(('LR', LogisticRegression()))
			models.append(('CART', DecisionTreeClassifier()))

			OR
			
			models.append(('LR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))
			models.append(('LR', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))

			# evaluate each model in turn
			results = []
			names = []
			score = "r2/neg_mean_squared_error/neg_mean_absolute_error/accuracy/neg_log_loss/roc_auc/f1/recall"
			for name, model in models:
				folds = KFold(n_splits = 5, random_state = 4)
				cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)
				results.append(cv_results)
				names.append(name)
				print("{0} - {1} ({2})".format(name, cv_results.mean(), cv_results.std()))
			
			# boxplot algorithm comparison
			fig = plt.figure()
			fig.suptitle('Algorithm Comparison')
			ax = fig.add_subplot(111)
			plt.boxplot(results)
			ax.set_xticklabels(names)
			plt.show()

	### preprocessing techniques

		## cap outliers
		def cap_outliers(array, sigma_value=3):
		    upper_limit = array.mean() + sigma_value*array.std()
		    lower_limit = array.mean() - sigma_value*array.std()
		    array[array<lower_limit] = lower_limit
		    array[array>upper_limit] = upper_limit
		    return array
		
		df[num_attributes] = df[num_attributes].apply(cap_outliers)

		## aggregate categorical variables
		df.groupby('categorical_variable').target.mean()        # replace the categories with the aggregated values

		## impute missing values

			# ways to deal with missing values
			# 1. delete rows/columns with a lot of missing values (>40%)
			# 2. impute with mean/median/mode
			# 3. in categorical columns, assign a unique category such as -1
			# 4. predict missing values using trees or knn

			# visualise missing values
			import missingno as msno
			msno.matrix(df)                                # matrix of missing values
			msno.bar(df)                                   # missing value bar chart
			msno.heatmap(df)                               # correlation heatmap - 1 means both variables are missing together; -1 means not missing together even in a single observation
			msno.dendrogram(df)                            # dendogram of variables

			# Imputer - for numeric columns
			from sklearn.preprocessing import Imputer
			imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
			imputer.fit(X_train)
			X_train = imputer.transform(X_train)
			X_test = imputer.transform(X_test)

			# CustomImputer - for numeric as well as categorical columns
			from sklearn.base import BaseEstimator, TransformerMixin
			class CustomImputer(BaseEstimator, TransformerMixin):
				def __init__(self, strategy='mode', filler='NA'):
			    	self.strategy = strategy
			    	self.fill = filler

				def fit(self, X, y=None):
					if self.strategy in ['mean','median']:
						if not all(X.dtypes == np.number):
							raise ValueError('dtypes mismatch np.number dtype is \
			                                 required for '+ self.strategy)
					if self.strategy == 'mean':
						self.fill = X.mean()
					elif self.strategy == 'median':
						self.fill = X.median()
					elif self.strategy == 'mode':
						self.fill = X.mode().iloc[0]
					elif self.strategy == 'fill':
						if type(self.fill) is list and type(X) is pd.DataFrame:
							self.fill = dict([(cname, v) for cname,v in zip(X.columns, self.fill)])
					return self

				def transform(self, X, y=None):
					return X.fillna(self.fill)

			imputer = CustomImputer(strategy = "mean/median/mode")
			imputer.fit(X_train)                           # use this imputer on numeric and categorical columns separately
			X_train = imputer.transform(X_train)
			X_test = imputer.transform(X_test)

			# fancy impute
			from fancyimpute import KNN
			imputer = KNN(k              =  5,             # number of neighboring rows to use for imputation
						  orientation    =  "rows",        # axis of the input matrix to be treated as a sample - "rows" or "columns"
						  print_interval =  100,           # integer
						  min_value      =  ,              # minimum possible imputed value
						  max_value      =  ,              # maximum possible imputed value
						  verbose        =  True           # boolean
			)
			X_complete = imputer.complete(X_incomplete)    # X_incomplete contains missing values

		## scale numeric columns

			# stanadard scaler - not to be used in machine learning pipelines
			from sklearn.preprocessing import scale
			X_scaled = scale(X)

			# MinMaxScaler
			from sklearn.preprocessing import MinMaxScaler
			min_max_scaler = MinMaxScaler(feature_range=(0, 1))
			min_max_scaler.fit(X_train)
			X_train_scaled = min_max_scaler.transform(X_train)
			X_test_scaled = min_max_scaler.transform(X_test)

			# StandardScaler (0-mean, 1-standard deviation)
			from sklearn.preprocessing import StandardScaler
			standard_scaler = StandardScaler()
			standard_scaler.fit(X_train)
			X_train_scaled = standard_scaler.transform(X_train)
			X_test_scaled = standard_scaler.transform(X_test)

			# Normalizer - useful when data is sparse with lots of zeros
			from sklearn.preprocessing import Normalizer
			normalizer = Normalizer()
			normalizer.fit(X_train)
			X_train_scaled = normalizer.transform(X_train)
			X_test_scaled = normalizer.transform(X_test)

		## encode categorical columns

			# LabelEncoder
			from sklearn.preprocessing import LabelEncoder
			label_encoder = LabelEncoder()
			label_encoder.fit(X_train_subset)
			X_train_subset = label_encoder.transform(X_train_subset)
			X_test_subset = label_encoder.transform(X_test_subset)

			# OneHotEncoder - only works with numeric data
			from sklearn.preprocessing import OneHotEncoder
			one_hot_encoder = OneHotEncoder()
			one_hot_encoder.fit(X_train_subset)
			X_train_subset = one_hot_encoder.transform(X_train_subset)
			X_test_subset = one_hot_encoder.transform(X_test_subset)

			# CategoricalEncoder
			from sklearn.preprocessing import CategoricalEncoder
			categorical_encoder = CategoricalEncoder(encoding='ordinal/onehot/onehot-dense')
			categorical_encoder.fit(X_train_subset)
			X_train_subset = categorical_encoder.transform(X_train_subset)
			X_test_subset = categorical_encoder.transform(X_test_subset)

			# binarizer - convert all values above the threshold to 1 and all values below the threshold to 0
			from sklearn.preprocessing import Binarizer
			binarizer = Binarizer(threshold=0.5).fit_transform(X)

		## pipeline

			from sklearn.pipeline import Pipeline              # alternative - import make_pipeline

			# all but last step need to have transform function. Last step may or may not have transform function.
			steps = [('impute', Imputer())
					 ("scaler", StandardScaler()),
					 ("model_name", model)]
			pipeline = Pipeline(steps)
			pipeline.fit(X_train, y_train)
			
			# predict - last item has to be a model
			y_pred = pipeline.predict(X_test)

			# transform - there should be no model in the pipeline
			X_train = pipeline.transform(X_train)
			X_test  = pipeline.transform(X_test)
			
			# multiple pipelines
			from sklearn.pipeline import FeatureUnion
			pipeline_1 = Pipeline([(),()])
			pipeline_2 = Pipeline([(),()])

			union = FeatureUnion([
						("p1": pipeline_1),
						("p2": pipeline_2)
					])
			final_pipeline = Pipeline([('union', union), (), (), ("model_name", model)])
			final_pipeline.fit(X_train, y_train)


			# extract model from pipeline
			model = final_pipeline.named_steps['model_name']

	### unsupervised learning

		## clustering

			# k-means clustering

				from sklearn.cluster import KMeans
				model = KMeans(n_clusters = )
				model.fit(X)
				model.inertia_
				model.cluster_centers_
				clusters = model.predict(X)
				new_labels = model.predict(X_new)

			# hierarchical clustering

				from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
				
				# complete linkage : distance between clusters is the distance between the farthest points of the clusters.
				# single linkage   : distance between clusters is the distance between the closest points of the clusters.
				mergings = linkage(X, method = "complete")
				clusters = fcluster(mergings, 15, criterion = "distance")  # dendrogram height = 15
				pd.DataFrame({"labels": y, "cluster": clusters})           # cluster number starts from index 1
				dendrogram(mergings, labels = y, leaf_rotation = , leaf_font_size = )  # visualize dendrogram
				plt.show()

		## dimensionality-reduction

			# t-SNE - visualize n-dimensional data in 2-dimensions

				from sklearn.manifold import TSNE
				model = TSNE(learning_rate = 100)  # optimum lr = [50,200]. Results should be such that points should appear in clusters i.e. not intermingled.
				transformed = model.fit_transform(X)
				plt.scatter(transformed.iloc[:, 0], transformed.iloc[:, 1], c = y)
				plt.show()

			# principle component analysis

				from sklearn.decomposition import PCA
				model = PCA(n_components = )       # first do model = PCA(). optimum n_components_ is known after plotting the bar plot - using elbow method
				model.fit(df)                      # scale data before applying pca
				df_pca = model.transform(df)
				
				np.round(pca.explained_variance_ratio_.cumsum(), 4)*100
				
				features = range(pca.n_components_)
				cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)
				plt.xticks(features)
				plt.plot(cumulative_variance)

				# pca for sparse matrices such as bag-of-words model or tf-idf model
				from sklearn.decomposition import TruncatedSVD
				model = TruncatedSVD(n_components = )
				model.fit_transform(sparse_df)     # sparse_df is scipy.sparse.csr_matrix, not a numpy array
				df_pca = model.transform(sparse_df)
				
				np.round(pca.explained_variance_ratio_.cumsum(), 4)*100

			## non-negative matrix factorization (NMF) - interpretable, unlike pca

				from sklearn.decomposition import NMF
				model = NMF(n_components = n)      # always specify n_components, unlike PCA()
				model.fit(df)                      # data should have non-negative entries only
				nmf_features = model.transform(df)
				model.components_

				# recommendor system using NMF
				nmf_features = normalize(nmf_features)
				df = pd.DataFrame(nmf_features, index = "article_names")
				current_article = df.loc("article_name")
				similarities = df.dot(current_article)  # cosine similarity. the more the similar
				print(similarities.nlargest())          # see similarities between current article and other articles. Recommend the most similar article.

==============================================================================================================================

