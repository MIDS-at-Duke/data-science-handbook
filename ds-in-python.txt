### numpy
	
	import numpy as np

		array = np.array([1, 2, 3, 4], dtype = )      # creates a numpy array
		array.shape                                   # print shape of array
		array.size                                    # number of elements
		array.itemsize                                # size of each element in memory
		array.dtype                                   # data type of array
		
		np.zeros((5,3))
		np.ones((5,3))
		np.empty((5,3))
		
		random.seed(4)                                # set seed for random number
		random.rand()                                 # create random number in [0.0,1.0)
		random.randint(low, high, size)               # create random integer between in [low, high)
		random.random((3, 4))                         # create 3x4 array with random numbers in [0.0,1.0)
		random.choice([0,1,2], size, replace, p)      # generate random numbers of size from array with probability p
		random.shuffle(array)                         # modify a sequence in-place by shuffling its contents
		
		np.arange(start, stop, step)                  # similar to seq() in R
		np.linspace(start, stop, num, endpoint=True)  # generate "num" elements between [start, stop]
		
		array.reshape((3, 4))                         # return an array of changed size
		array.resize((3, 4))                          # changes the array in-place
		array.transpose()                             # return transpose of matrix
		array.sum(axis = 0/1)                         # 0 - column, 1 - row
		array.cumsum(axis = 0/1)                      # cumulative sum across column/row
		
		np.exp(array)                                 # array or pandas Series
		np.sqrt(array)
		np.max(array, axis)
		np.min(array, axis)
		np.mean(array)
		np.median(array)
		np.var(array)
		np.std(array)
		np.percentile(array, [10, 40, 90])            # similar to quartile() in R
		np.cov(x, y)
		np.corrcoef(x, y)                             # pearson correlation coefficient

		column_stack((a, b))  						  # similar to cbind in R. 
		row_stack((a, b))                             # similar to rbind in R
		vstack((a, b))                                # vertically stack array a and b.
		hstack((a, b))         						  # horizontally stack array a and b
		concatenate((a, b), axis = )                  # concatenate array a and b

		a = array([1, 2, 3, 4])
		b = a                 						  # not a copy. If "b" changes, "a" changes
		c = a.copy()           						  # deep copy. If "c" changes, "a" remains unchanged

		# indexing and slicing
		array[start:end:jump, start:end:jump]         # for 2D array

		# boolean operations
		logical_and(array1, array2)                   # returns an array with boolean values
		logical_or(array1, array2)
		logical_not(array1, array2)
	
		# distributions - google numpy distribution functions
		np.random.normal(mean, std, size)
		np.random.binomial(n, p, size)
		np.random.poisson(n, size)
		np.random.exponential(tau, size)

		# linear regression in numpy
		slope, intercept = np.polyfit(x, y, 1)        # degree of polynomial is 1 to specify linear regression

		# print options
		from numpy import set_printoptions
		set_printoptions(precision=3)

==============================================================================================================================

### pandas

	import pandas as pd

		# print options
		pd.set_option("display.max_columns", 20)  # set display limit to all columns
		pd.get_options("display.max_rows")
		pd.reset_options("display.max_columns")
		
		# read data
		df = pd.read_csv(file_name,
						 sep          =   ,         # character that seprates different values in the file
						 names        = [],         # pass column names explicitly
						 index_col    =   ,         # column index to use as index
						 dtype        =   ,         # refer documentation
						 na_values    =   ,         # list of values to be read as na
						 parse_dates  =   ,         # refer documentation
		)
		
		df = pd.DataFrame(data        =   ,         # numpy ndarray or python dictionary
						  columns     = [],         # list of column names
		)
		
		df.shape                                    # return shape as tuple
		df.head(n)                                  # return first "n" rows of df
		df.tail(n)                                  # return last "n" rows of df
		df.dtypes                                   # return data type of each column
		df.describe()                               # similar to summary() in R
		df.info()

		# check missing values
		df.isnull().any()                           # return True if any of the values is missing
		df.isnull().sum()                           # check number of missing values
		df.notnull().all()                          # return True only if no missing values prsent

		# sample rows or columns
		df.sample(n             =   ,               # number of rows/columns to sample
				  frac          =   ,               # either use n or frac
				  replace       = False,            # sample with ro without replacement
				  random_state  =  4,
				  axis          =   ,               # sample either row or columns of dataframe
		)

		# statistics
		df.corr(method="pearson")                   # return correlation matrix
		df.skew()                                   # return skewness of numeric variables
		df.categorical_column.nunique()             # return number of levels in a categorical variable
		df.categorical_column.value_counts(dropna = False)  # similar to table(df$category) in R
		df["column_name"].max()
		df["column_name"].mean()
		df["column_name"].mode()
		df["column_name"].std()                     # standard deviation of column
		df.column_name.quantile([0.1, 0.5])         # return quantile value
		df.column_name.cumsum()                     # cumulative sum
		df.column_name.cumprod()                    # cumulative product
		
		# visualisations
		df.hist()
		df.plot(kind="bar", x = , y = )             # refer documentation
		df.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
		df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)
		
		from pandas.tools.plotting import scatter_matrix
		scatter_matrix(df)
		

		df.select_dtypes(include=["int64"], exclude=["float64"])  # select columns with specific data types
		
		df.index = index_variable                   # index variable must be equal to the number of rows in the dataframe
		df.index.name = "index name"
		
		# conditions
		df[df.age > 10]
		df[df.age == df.age.max()]

		# filtering
		df[df.age.isin([13, 14, 15, 16, 17, 18, 19])]  # return teenagers
		df[(df.age < 13) & (df.age > 19)]              # return non-teenagers
		df[df.name == "Aman" | df.name = "Arjun"]      # return Aman and Arjun both
		df.all()                                       # return True if all the elements are True
		df.any()                                       # return True if any of the elements is True

		# apply functions to dataframe
		df.apply(function, axis = )                    # iterate over columns of dataframe
		df.column_name.apply(lambda x: x**2)           # iterate over values of column
		df.age.apply(np.argmax, axis = 1)              # max to get max value, np.argmax to get row name with max value

		# rename columns
		df.rename(columns = {"old_column_name":"new_column_name"})  # rename specific columns
		df.columns = ["column1", "column2"]            # rename all columns

		# loc, iloc
		df.loc[1:5,:]                               # print row [1,5]. need to specify the row and column header names
		df.loc[[1, 4, 7], ["column1", "column2"]]
		df.loc[1:10, "column1":"column2"]
		df.loc[age == df.age.max(), :]
		df.iloc[1:10, 2:4]                          # print row [1, 10). need to specify the row and column indices

		# hierarchical indexing
		df.set_index(["school", "rollno"], inplace = True)
		df.sort_index()
		df.loc["dgv":"dav"]                         # outer indexing (on "school")
		df.loc[1:10]  		                        # inner indexing (on "rollno")
		df.loc[(["dgv", "dav"], slice(1, 10)), :]   # colon slicing not permitted inside tuple, instead use slice()
		df.reindex()                                # conform data frame to new index. arrange rows or add new rows
		
		# fill NAs
		df.column_name.fillna(df.column_name.mean(), axis = "index or columns", method = "ffil/bfil", limit = 1)
		df.fillna(df.mean())
		df.fillna({
			"column_name": value,
			"column_name": value
		})

		df.interpolate(method = "linear")

		df.dropna(subset = ["list of columns"], how = "any/all", thresh = )

		# replace values
		replace = {"gender":          {"male": 0, "female": 1},
                   "marital_status":  {"married": 0, "divorced": 1, "single": 2}
		}
		df.replace(replace, inplace = True)
		df.replace({"old_value": "new_value", "old_value": "new_value"})   # mapping
		df.replace(["old_value_list"], ["new_value_list"])                 # mapping

		# group by
		group = df.groupby("categorical_column")    # returns GroupBy object
		group.function()                            # apply function to each group separately

		# group by and summarize
		df.groupby("categorical_column").another_column.function_name()  # count(), min(), max(), sum(), mean()
		df.groupby(["categorical_columns"]).agg({
			"column": [max, min], 
			"column": [size, "count"]               # size and "count" are same. "count" is a numpy function. see numpy functions
		})
		
		df.groupby("categorical_column").another_column.transform(function_name)  # makes changes to the data inplace

		df.groupby(["categorical_column", boolean_filter]).another_column.function_name()
		
		# change GroupBy to DataFrame
		df = pd.DataFrame(df.groupby())
		df = df.add_suffix('_Count').reset_index()

		# set column names after groupby
		df.columns = ["_".join(colname) for colname in df.columns]
		
		# sort dataframe
		df.sort_values(by = ["column_name", "column_name"], ascending = False)

		# Melt: wide to long dataframe
		pd.melt(df,
				id_vars = ["columns_to_keep"],
				value_vars = ["columns to melt"],   # optional, if not provided, function will melt all vars except id_vars
				var_name = ,  					    # new categorical column name
				value_name =                        # new numerical column name
		)  										    # similar to gather in tidyr
		
		# Unmelt: long to wide dataframe
		df.pivot_table(index = ,                    # columns not to unmelt
					   columns = ,                  # categorical column name
					   values =                     # numerical column name
		)  						                    # in case of duplicate values, use aggfunc to aggregate

		# concatenate dataframes - column + column or row + row
		pd.concat(objs         =  [df1, df2],
				  axis         =  ,                 # 0-index+index, 1-column+column
				  ignore_index = False              # keep index names
		)

		# merge/join dataframe
		df.merge(df1, df2, on = "common_column", how = "inner")

		# stacking and unstacking
		df.stack()
		df.stack.unstack()

		# crosstabs
		pd.crosstab(df.categorical_column1, df.categorical_column2,    # could pass column lists as well
					 margins = True,
					 normalize = {0, 1} or {"all", "index", "columns"}  # similar to prop.table() in R
		)  # similar to table(category1, catedory2) in R

		pd.crosstab(df.categorical_column1, df.categorical_column2
					 values = df.numerical_column,
					 aggfunc = np.average
		)  # similar to group by and summarize

		# find duplicate rows
		df.duplicated()                                # boolean ouput
		df.duplicated().sum()                          # return number of duplicate entries

		df.drop_duplicates(keep = "first", inplace = True)  # keep first entry and delete rest of the duplicate entries

		# drop column
		df.drop("column_name", axis = 1)

		# one hot encoding
		df_dummy = pd.get_dummies(df,
								  columns = [column for column in categorical_columns if column not in ["target_column"]],
								  prefix = 
				   )

		# select dataframe based on column names
		pattern = r"column name pattern"
		column_names_bool = list(df.columns.map(lambda x: bool(re.search(pattern, x))))
		column_names = list(df.columns[column_names_bool])
		new_df = df.loc[:, column_names]

==============================================================================================================================

### matplotlib

	import matplotlib.pyplot as plt

		# line plot
		plt.plot(x, y, marker = "", linestyle = "")
		
		# histogram
		plt.hist(x, bins = 30)                         # or pass a list of bin cutoff points
		
		# scatter plot
		plt.scatter(x, y, s = "size_variable", c = "color_variable", alpha = )
		
		# boxplot
		plt.boxplot(x)

		# 2-D histogram
		plt.hist2d(x, y, bins = (30, 30), range=((xmin, xmax), (ymin, ymax)))  # range used for zooming
		plt.colorbar()
		plt.show()

		plt.hexbin(x, y, gridsize=(), extent=(xmin, xmax, ymin, ymax))         # extent similar to range in hist2d()
		plt.colorbar()
		plt.show()

		# plotting images
		image = plt.imread("image.jpg")
		plt.imshow(image, extent = , aspect = )
		plt.axis("off")
		plt.show()

		collapsed = image.mean(axis=2)                 # average out all pixel intensities to get one channel color instead of three
		plt.set_cmap("gray")                           # change printing color to black and white
		plt.imshow(collapsed, cmap="gray")
		plt.axis("off")
		plt.show()

		# customization
		plt.xlabel("x")                                # use of latex equations possible
		plt.ylabel("y")
		plt.title("title")
		plt.xticks()                                   # customize x-axis ticks
		plt.yticks()                                   # customize y-axis ticks
		plt.tick_params()
		
		# transformation
		plt.xscale("log")                              # transform x-axis to log scale
		plt.yscale("log")                              # transform y-axis to log scale

		plt.show()                                     # display plot
		plt.clf()                                      # clear plot

		# multiple plots using axes()
		plt.axes([x_lo, y_lo, width, height])          # all parameters between 0 and 1
		plt.plot(x, y, color = "red")
		plt.axes([x_lo, y_lo, width, height])
		plt.plot(a, b, color = "blue")
		plt.show()

		# multiple plots using subplot()
		plt.subplot(nrows, ncols, nsubplot)            # all parameters indexed from 1
		plt.plot(x, y, color = "red")
		plt.subplot(nrows, ncols, nsubplot)            # activate nth subplot
		plt.plot(a, b, color = "blue")
		plt.tight_layout()                             # pad space between two plots
		plt.show()

		# zooming and axis
		plt.xlim([xmin, xmax])  	                   # zoom in
		plt.ylim([ylim, ymax])
		plt.axis([xmin, xmax, ymin, ymax])
		plt.axis("off/equal/square/tight")
		plt.figure(figsize=(10,10))

		# legends
		plt.scatter(x, y, color = "green", label = "male")
		plt.scatter(x, y, color = "red", label = "female")
		plt.legend(loc = "upper right")                # google legend locations
		plt.show()

		# text and arrows
		plt.annotate("text",
					 xy = (10, 10),                    # text coordinates
					 xytext = (15, 15),		           # arrow pointer coordinates
					 arrowprops = {"color": "green"}   # arrow properties
		)

		OR
		
		plt.text(x, y, s = "text to display on x,y coordinates")  # display text on x,y point
		plt.grid(True)                                 # to enable the text

		# themes and styles
		plt.style.available
		plt.style.use("ggplot")

==============================================================================================================================

### seaborn

		import seaborn as sns

	## univariate plots

		# regression line
		sns.lmplot(x = "column_name", y = "column_name", hue = "categorical_column", data = df, palette = "Set1")
		sns.lmplot(x = "column_name", y = "column_name", col/row = "categorical_column", data = df)  # two plots based on gender
		plt.show()

		# higher order regression line - polynomial regression
		sns.regplot(x = , y = , data = auto, color = "color_name", order = 2, scatter_kws = {"alpha": 0.5, "s":2})  # s-size
		sns.regplot(x = , y = , data = auto, color = "color_name", order = 3, scatter = None)  # scatter = None avoids plotting the points again
		plt.show()

		# residual plot
		sns.residplot(x = , y = , data = , color = "color_name")

		# strip plot
		mycolors = sns.color_palette("seaborn palette names or matplotlib colormap or list of colors", n_colors = )  # return n_colors
		sns.stripplot(x = "categorical_column",                 # optional
					  y = "numeric variable",
					  data = df,
					  jitter = True,                            # same as in ggplot2
					  hue = "categorical_column",
					  orient = "v/h",                           # change orientation to vertical/horizontal
					  size = ,                                  # point size
					  palette = {"male": "g", "female": "r"},   # color correspoding to categorical value of variable
					  palette = mycolors                        # color for each value of x or hue
		)

		# swarm plot
		sns.swarmplot("same arguments as strip plot")  # visualize repeated value differently

		# boxplot
		sns.boxplot("same arguments as strip plot")
		
		# violin plot
		sns.violinplot("same arguments as strip plot")
		
	## bivariate plots

		# joint plot
		sns.jointplot(x = "numeric_column", y = "numeric_column", data = df, kind = "kde")

		# pair plot
		sns.pairplot(df)  # pair plots between all numeric variables
		sns.pairplot(df, hue = "categorical_column", kind = )

		# covariance heat map
		mycmap = sns.diverging_palette(h_neg=20, h_pos=220, s=75, l=50, sep=10, center='light', as_cmap=True)
		sns.heatmap(df.corr(), cmap = mycmap, annot = True)

		# ECDF plot - make custom function
		x = np.sort(df.column_name)
		y = np.arange(1, len(x) + 1) / len(x)
		plt.plot(x, y, marker = ".", linestyle = "none")
		plt.margins(0.02)  # keep data off plot edges

==============================================================================================================================

### bokeh

		from bokeh.io import output_notebook, show
		from bokeh.plotting import figure

		# glyphs - shapes like markers, lines, wedges and patches. google bokeh glyphs
		plot = figure(plot_width = 400, tools = "pan, box_zoom")  # initialize empty plot
		plot.circle(x = [1,2,3,4,5],			# sequences, numpy arrays, pandas dataframes supported (df.column_name)
					y = [8,6,5,2,3],			# sequences, numpy arrays, pandas dataframes supported (df.column_name)
					x_axis_label = "",
					y_axis_label = ""
					color = ,
					size = ,
					alpha = ,
					legend = ,					# label to display on legend when plotting multiple plots on same figure
					source = 				    # in case you are passing source only mention column names in x and y
		)  # draw on plot
		output_file("circle.html")  # plot to be output in a web browser. Use if output_file is imported.
		show(plot)  # display the plot

		# glyphs accept python sequences for every parameter. If single value is provided instead of a sequence, it is used throughout.
		plot.circle([1,2,3,4,5], [8,6,5,2,3], size = [1,2,3,4,5])  # different size for each circle

		# add multiple markers over each other
		plot.x()
		plot.circle()
		plot.triangle()
		show(plot)

		# lines
		plot.line(x = , y = , line_width = )

		# patches - useful to draw geographic regions
		plot.patches(x = [[1,2,3,4], [1,2], [1,2,3]],
					 y = [[7,5,3,6], [5,1], [8,3,6]],
					 color = ["red","blue","green"],
					 line_color = "white"
		)

		# ColumnDataSource - main data frame central to bokeh
		from bokeh.plotting import ColumnDataSource

		cds = ColumnDataSource(df)
		plot.circle(x = "column_name", y = "column_name", source = cds)

	## animations

		# selection appearance
		plot = figure(tools = "box_select, lasso_select")
		plot.circle(x = ,
					y = ,
					selection_color = "red",
					nonselection_fill_alpha = ,
					nonselection_fill_color = 
		)

		# hover appearence
		from bokeh.models import HoverTool

		hover = HoverTool(tooltips = None, mode = "hline")
		plot = figure(tools = [hover, "crosshair"])
		plot.circle(x = ,
					y = ,
					hover_color = "red"
		)

		# color mapping
		from bokeh.models import CategoricalColorMapper

		mapper = CategoricalColorMapper(factors = ["male", "female"], palette = ["red", "green"])
		plot.circle(x = "column_name",
					y = "column_name",
					source = cds,
					color = {"field": "gender",
							 "transform: mapper"
					}
		)

	## layouts

		# row/column layout
		from bokeh.layouts import row, column
		layout = row(p1, p2, p3)     # arrange in row
		layout = column(p1, p2, p3)  # arrange in column
		output_file("layout.html")
		show(layout)

		# nested row/column layout
		column_layout = column(p1, p2)
		nested_layout = row(column_layout, p3, sizing_mode='scale_width')

		# grid layout
		from bokeh.layouts import  gridplot
		layout = gridplot([p1, p2], [p3, None], toolbar_location = None)  # same output as the nested row-column shown above

		# tabbed layout
		from bokeh.models.widgets import Tabs, Panel
		
		first = Panel(child = row(p1, p2), title = "first")
		second = Panel(child = row(p3), title = "second")

		tabs = Tabs(tabs = [first, second])
		show(tabs)

		# linking plots

	## annotations and guide
		
		# legend locatiion
		plot = plot.circle()
		plot.legend.location = "top_left"
		plot.legend.background_fill_color = 'lightgray'      # google more legend properties

		# hover tooltips
		from bokeh.models import HoverTool

		hover = HoverTool(tooltips = [
				("label", "@column_name"),                   # ("length of petal", "@petal_length")
				("label", "@column_name"),
				("label", "@column_name")
				])
		
		plot = figure(tools = [hover, "pan", "wheel_zoom"])  # or plot.add_tools(hover)
		show(plot)

	## charts

		# histogram
		from bokeh.charts import Histogram  # google bokeh charts
		plot = Histogram(df, "column_name", bins = , color = "categorical_column", title = )
		p.xaxis.axis_label = ""
		p.xaxis.axis_label = ""
		show(plot)

		# boxplot
		from bokeh.charts import BoxPlot
		plot = BoxPlot(df, values = "column_name", label = "categorical_column", color = "categorical_column", title = "")

		# scatter plot
		from bokeh.charts import Scatter
		plot = Scatter(df, x = "column_name", y = "column_name", color/marker = "categorical_column", title = "")

==============================================================================================================================

### python

		# print
		print(*objects, sep=' ', end='\n', file=sys.stdout, flush=False)
		print("Value of x is: {1} and value of y is: {0}".format(y, x))
		print("Value of x is:", x, "and value of y is:", y)

		# if-else condition
		if condition:
			code
		elif condition:
			code
		else:
			code

		# while loop
		while condition:
			code

		# for loop
		for var in seq:
			print(var)

		for index, var in enumerate(seq):
			print(str(index) + "-" + str(var))

		for x, y, z in zip(sequence1, sequence2, sequence3):
			print(x, y, z)

		# for loops for dictionaries
		for key, value in dict.items():
			print(str(key) + "-" + str(value))

		# for loops for numpy arrays
		for var in np.nditer(numpy_array):
			print(var)

		global var    # use global variable instead of local variable inside a function
		nonlocal var  # in case of nested functions, access the var present in outer function instead of local function

		# default arguments
		def function_name(x = 0):
			print(x)

		# multiple arguments
		def function_name(*args):
		""" print all the parameters """
			for param in args:
				print(param)
		function_name(1, 2, "hi")

		def functions_name(**kwargs):                 # keyword args
		""" print all the key (parameter name) value (parameter value) pairs passed to the function """
			for key, value in kwargs.items():
				print(str(key) + ":" + str(value))
		function_name(first_name = "Amandeep", last_name = "Rathee")

		# lambda functions
		lambda variables: operations                  # return result of operation
		square = lambda x: x**2                       # return square of x

		map(function, sequence)                       # useful for applying lambda functions to sequences like lists. extract reults by list(map())
		filter(function, sequence)                    # use to filter out elements of sequence. access the result using list(filter())
		from functools import reduce
		reduce(function, sequence)                    # returns a single value unlike map() and filter()

		# error handling
		def root(number):
			if root < 0
				raise ValueError("negative numbers not allowed.")

			try:
				return number ** 0.5
			except:
				print("Please enter an int or float.")

		# iterators and iterables
		iterator = iter(iterable)                     # iterable can be any sequence-lists, string, dictonaries etc.
		next(iteration)                               # print the next item in iterable

		list(enumerate(sequence, start = 0))          # return a list of tuples with index and the item of sequence
		list(zip(sequence1, sequence2, sequence3))    # return a list of tuples. each tuple has corresponding items from each sequence

		# load dataframe in chunks
		for df in pd.read_csv("", chunksize = n)      # load "n" rows in chunks. use next(df) to iterate over chunks

		# list comprehension
		squares = [num**2 for num in range(5)]        # produce a list of squares of 1 through 5
		squares = [num**2 for num in range(5) if num % 2 == 0]            # return squares only for even numbers. condition for iterable
		squares = [num**2 if num % 2 == 0 else 0 for num in range(5)]     # return square if number is even else 0. condition for iterator
		pairs = [(num1, num2) for num1 in range(5) for num2 in range(5)]  # return a list of tuples

		# dictionary comprehension
		result = {number: number**2 for number in range(5)}

		# generators
		generator = (num for num in range(10))        # generator can be iterated over. instead of returning a list at once it returns number as and when required.
		def generator_function(n):
			""" return numbers upto n """
			i = 0
			while i < n
			yield i                                   # generates i and returns as the while loop runs
			i += 1

	# importing data

		# import text file
		file = open("filename.txt", mode = "r")       # open file in read-only mode
		fulltext = file.read()
		firstline = file.readline()                   # read line iteratively
		file.close()
		print(file.closed)                            # check if file is closed

		OR

		with open("filename.txt", "r") as file:
			print(file.readline())

		# import flat file using numpy
		np.loadtxt("filename", delimiter = ",", skiprows = 1, usecols = [1:10], dtype = )
		np.genfromtxt("filename", delimiter = ",", names = True, dtype = None)
		np.recfromcsv("filename")                     # same as genfromtxt() with parameters shown above as default

		# import flat file using pandas
		df = pd.read_csv("filename", nrows = , header = , sep = "", names = ["column names"],
						 comment = ["", ""], na_values = ["", ""], parse_dates = , index_col = )

		array = df.values                             # extract numpy array from dataframe

		# pickled files - python specific
		import pickle
		
		with open("data.pkl", "rb") as file:          # rb - readonly binary
    			  data = pickle.load(file)
    	print(data)

		# import excel file
		df = pd.ExcelFile("filename")
		df.sheet_names()  # print sheet names
		df.parse("sheetname", parse_cols = 0, skiprows = 0:2, names = [""])        # extract sheet by specifying sheet name
		df.parse(sheet_index, parse_cols = 0:2, skiprows = [0], names = ["", ""])  # extract sheet by specifying sheet index

==============================================================================================================================

### web scraping

		from urllib.request import urlretrieve
		urlretrieve("url", "filename.csv")            # save url file to csv

		pd.read_csv("url", sep = )
		pd.read_excel("url", sheetname = None)        # read all sheets of excel file from url as a dictionary with sheet names as keys
		
==============================================================================================================================

### data cleaning

		df.column.str[0]                              # extract first letter from df.column
		df.column.str.split("_")                      # split column on "_"
		df.column.str.get(0)                          # get 0th item from each value of the column

		# importing files from current directory
		import glob
		files = glob.glob("*.csv")                    # return list of all csv file names from current working directory

		df.dtypes                                     # get types of each column of dataframe
		df.column.astype(str/"category")              # change column to string/categorical variable
		pd.to_numeric(df.column, errors = "coerce")   # change column to numeric

		# string matching
		## to extract "I have $1767.89 and that"s a lot of money", use - "^\$[0-9]*\.[0-9]{2}$"
		import re
		pattern = re.compile("^\$[0-9]*\.[0-9]{2}$")
		result = pattern.match("I have $1767.89 and that is a lot of money!")
		re.split(pattern, string)                     # split string at given pattern
		re.findall("pattern", "string")               # return all matched patterns
		df.column.replace(",", ";")                   # replace "," with ";" 
		df.column.str.contains("pattern")             # return True if pattern found in column

		# To remove $ sign from the column
		df.amount.apply(lambda x: x.replace("$", ""))
		df.amount.apply(lambda x: re.findall("\d+\.\d+", x)[0])  # if [0] is not used result will be [28.45] instead of 28.45

		# assert
		assert condition                              # if condition is true, do nothing, if false, throw error

		# write csv
		df.to_csv("filename.csv", sep = )

==============================================================================================================================

### time series with pandas

		# resampling - change (increase - upsampling or decrease - downsampling) frequency of timeseries data
		# frequencies - T(minute), H(hour), D(day), B(business day), W(week), M(month), Q(quarter), A(year)

		frequency = "2Q"                   # change frequency to half yearly - upsample
		df.resample(frequency).function()  # statistical methods - mean(), sum(), count(), to fill NaN - interpolate()

		# date methods
		df.data_column.dt.hour             # extract hour
		df.data_column.dt.dayofweek        # extract dayofweek
		df.data_column.dt.dayofyear        # extract dayofyear
		df.data_column.dt.weekofyear       # extract weekofyear
		df.data_column.dt.month            # extract month
		df.data_column.dt.year             # extract year

		# visualize time series
		df.column.plot(style = "k.-")      # google styling in matplotlib plot. color(k:black), marker(.:dot), line type(-:solid)
		
==============================================================================================================================
