
### scikit-learn

	## supervised learning

		# make artificial dataset
		from sklearn.datasets import make_classification
		X, y = make_classification(n_samples = , n_features = ,
								   n_informative = , n_redundant = ,
								   n_repeated = , random_state = ,
								   weights = [0.5, 0.5]
			   )

		# divide data into X and y
		X = df.drop("target_column", axis = 1).values
		y = df.target_column.values
		
		# encode class values as integers
		from sklearn.preprocessing import LabelEncoder
		encoder = LabelEncoder()
		encoder.fit(y)
		encoder.classes_
		encoded_y = encoder.transform(y)
		y = encoder.inverse_transform(encoded_y)

		# split data into train and test
		from sklearn.model_selection import train_test_split
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)
		
		# cross validation
		from sklearn.model_selection import KFold
		from sklearn.model_selection import StratifiedKFold  # useful in case of imbalanced class
		from sklearn.model_selection import LeaveOneOut
		from sklearn.model_selection import ShuffleSplit     # multiple train/test sets
		from sklearn.model_selection import cross_val_score
		
		folds = KFold(n_splits = 5, shuffle = True, random_state = 4)
		folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)
		folds = LeaveOneOut()
		folds = ShuffleSplit(n_splits = 5, test_size = 0.3, random_state = 4)

		score = "r2/neg_mean_squared_error/neg_mean_absolute_error/accuracy/neg_log_loss/roc_auc/f1/recall"
		cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)  # pass model/pipeline after instantiating it
		cv_results.mean()*100
		cv_results.std()*100

		# manual cross validation
		for train_index, test_index in folds.split(X_train, y_train):
			model.fit(X_train[train_index], y_train[train_index])
			y_pred = model.predict(X_train[test_index])
			model.score(X_train[test_index], y_train[test_index])

		# hyperparameter tuning
		from sklearn.model_selection import GridSearchCV
		from sklearn.model_selection import RandomizedSearchCV
		params = {"parameter_name": [parameter_values]}
		
		model_cv = GridSearchCV(estimator   = model,       # pass model/pipeline after instantiating it
								param_grid  = params,
								scoring     = score,
								cv          = folds,
								verbose     = 1            # the higher, the more messages
					)
		
		model_cv = RandomizedSearchCV(estimator             = model,
									  param_distributions   = params,
									  n_iter                = 100,
									  scoring               = score,
									  cv                    = folds,
									  verbose               = 1,
									  random_state          = 4
					)
		model_cv.fit(X_train, y_train)                     # search takes place here
		model_cv.cv_results_
		model_cv.best_score_
		model_cv.best_params_

	## models

		# linear regression
		from sklearn.linear_model import LinearRegression
		model = LinearRegression()                         # instantiate model
		model.fit(X_train, y_train)                        # fit model
		y_pred = model.predict(X_test)                     # predict using model
		model.score(X_test, y_test)                        # evaluate model

		# regularization
		from sklearn.linear_model import Lasso             # l1-regularization
		model = Lasso(alpha = , normalize = )
		model.fit(X_train, y_train)
		coefficients = model.fit(X_train, y_train).coef_   # extract coefficients for each feature
		plt.plot(range(len(names)), coefficients)          # variable importance
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		from sklearn.linear_model import Ridge             # l2-regularization
		model = Ridge(alpha = , normalize = )
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		from sklearn.linear_model import ElasticNet        # mix of l1 and l2 regularization
		model = ElasticNet(alpha = , normalize = )
		model.fit(X_train, y_train)
		coefficients = model.fit(X_train, y_train).coef_   # extract coefficients for each feature
		plt.plot(range(len(names)), coefficients)          # variable importance
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		# logistic regression
		from sklearn.linear_model import LogisticRegression
		from sklearn.feature_selection import RFE
		model = LogisticRegression()
		# rfe = RFE(model, n)                              # recursive feature elimination. select top n features
		# fit = rfe.fit(X_train, y_train)
		# print(fit.n_features_, fit.support_, fit.ranking_)
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		# multiclass logistic regression
		from sklearn.multiclass import OneVsRestClassifier
		model = OneVsRestClassifier(LogisticRegression())  # also used when the target column is changed to dummy target columns

		# knn
		from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
		model = KNeighborsClassifier(n_neighbors = 6)
		model = KNeighborsRegressor()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		# support vector machines
		from sklearn.svm import SVC, SVR
		model = SVC(C = , kernel = "")
		model = SVR(C = , kernel = "")
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)

		# decision tree
		from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz
		import graphviz
		model = DecisionTreeClassifier(criterion  = "gini/entropy",
									   max_depth  = ,
									   min_samples_split = ,
									   min_samples_leaf = ,
									   max_features = ,
									   max_leaf_nodes = ,
									   min_impurity_decrease = ,
									   class_weight = "balanced",
									   presort = "True"                   # faster training when daset is small
				)
		model = DecisionTreeRegressor()  # same parameters as DecisionTreeClassifier()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)
		model.feature_importances_
		model.tree_
		graph_data = export_graphviz(model, 
									 out_file            =   None,
									 feature_names       =   ,
									 class_names         =   ,
									 filled              =   True,
									 rounded             =   True,  
									 special_characters  =   True
					 )
		graph = graphviz.Source(graph_data) 
		graph.render("tree")  # save tree as tree.pdf file

		# random forest
		from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
		model = RandomForestClassifier(n_estimators        = ,            # number of trees
									   max_features        = ,            # number of features at each node split
									   oob_score           = True,
									   random_state        = 4,           # random seed

				)
		model = RandomForestRegressor()          # same parameters as RandomForestClassifier()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)
		model.feature_importances_
		model.oob_score_

		# ada boost
		from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
		model = AdaBoostClassifier(base_estimator    = ,                  # optional
								   n_estimators      = 100,               # number of trees
								   learning_rate     = 0.1,               # trade-off between learning_rate and n_estimators
								   random_state      = 4,                 # random seed

				)
		model = AdaBoostRegressor()              # same parameters as RandomForestClassifier()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)
		model.feature_importances_
		model.estimator_errors_
		model.estimator_weights_

		# gradient boosting
		from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
		model = GradientBoostingClassifier(loss                = "deviance",
										   learning_rate       = 0.1,
										   n_estimators        = 100,         # number of trees
										   max_depth           = 3,           # depth of tree
										   min_samples_split   = 2,
										   min_samples_leaf    = 1,
										   subsample           = 1.0,
										   max_features        = None,
										   verbose             = 0,
										   random_state        = 4,

				)
		model = GradientBoostingRegressor()             # same parameters as GradientBoostingClassifier()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		model.score(X_test, y_test)
		model.feature_importances_
		model.train_score_[i]                           # train score of i-th iteration

		# xgboost
		from xgboost import XGBClassifier
		model = XGBClassifier()
		model.fit(X_train, y_train)
		y_pred = model.predict(X_test)
		predictions = [round(value) for value in y_pred]
		model.score(X_test, y_test)
		
		OR

		import xgboost as xgb
		dtrain = xgb.DMatrix(X_train, label = y_train)
		dtest = xgb.DMatrix(X_test, label = y_test)
		parameters = [
      # General Parameters
          booster            = "gbtree",          		# default = "gbtree"
          silent             = 0,                 		# default = 0
      # Booster Parameters
          eta                = 0.3,               		# default = 0.3, range: [0,1]
          gamma              = 0,                 		# default = 0,   range: [0,∞]
          max_depth          = 6,                 		# default = 6,   range: [1,∞]
          min_child_weight   = 1,                 		# default = 1,   range: [0,∞]
          subsample          = 1,                 		# default = 1,   range: (0,1]
          colsample_bytree   = 1,                 		# default = 1,   range: (0,1]
          colsample_bylevel  = 1,                 		# default = 1,   range: (0,1]
          lambda             = 1,                 		# default = 1
          alpha              = 0,                 		# default = 0
      # Task Parameters
          objective          = "binary:logistic", 		# default = "reg:linear"
          eval_metric        = ["error", "logloss"],
          num_classes        = 2,                 		# number of classes in case of multi class classification
          seed               = 4				  		# reproducability seed
        ]

        eval_list  = [(dtest,'eval'), (dtrain,'train')]
        model = xgb.train(parameters, dtrain, num_round = 10, eval_list, early_stopping_rounds = 10)
        y_pred = model.predict(dtest, ntree_limit = model.best_ntree_limit)  # pass ntree_limit only if early_stopping_rounds used
        xgb.plot_importance(model)                      # import matplotlib for this
        xgb.plot_tree(model, num_trees=2)               # import graphviz for this
        model.save_model("model_name")

        # voting ensemble
        from sklearn.ensemble import VotingClassifier
        estimators = []
        estimators.append(("knn", KNeighborsClassifier()))
        estimators.append(('cart', DecisionTreeClassifier()))
        estimators.append(('svm', SVC()))
        ensemble = VotingClassifier(estimators)
        cv_results = cross_val_score(ensemble, X_train, y_train, cv = folds, scoring = score)

    ## save/load model
    	
    	# the python version and library versions almost certainly need to be the same while loading a saved model

        # save model to disk
        from pickle import dump
        dump(model, open("filename.sav", 'wb'))

        from sklearn.externals.joblib import dump
        dump(model, "filename.sav")

        # load model from disk
        from pickle import load
        model = load(open("filename.sav", 'rb'))
        
        from sklearn.externals.joblib import load
        model = load("filename.sav")

	## evaluate model performance

		# return default metric
		model.score(X_test, y_test)                     # return accuracy/r-squared
		
		# regression

		from sklearn.metrics import r2_score
		r2_score(y_test, y_pred)

		from sklearn.metrics import mean_squared_error
		mean_squared_error(y_test, y_pred)

		# classification

		from sklearn.metrics import accuracy_score
		accuracy_score(y_test, y_pred)

		from sklearn.metrics import confusion_matrix
		confusion_matrix(y_test, y_pred)

		from sklearn.metrics import classification_report
		classification_report(y_test, y_pred)

		from sklearn.metrics import roc_curve
		y_pred_prob = model.predict_proba(X_test)[:, 0/1]  # return probabilites of class 0/1
		fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

		from sklearn.metrics import roc_auc_score
		y_pred_prob = model.predict_proba(X_test)[:, 0/1]
		roc_auc_score(y_test, y_pred_prob)

		from sklearn.metrics import log_loss
		log_loss(y_test, y_pred)                           # see scikit-learn documentation

	## compare machine learning models

		# prepare models
		models = []
		models.append(('LR', LogisticRegression()))
		models.append(('CART', DecisionTreeClassifier()))

		OR
		
		models.append(('LR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))
		models.append(('LR', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))

		# evaluate each model in turn
		results = []
		names = []
		score = "r2/neg_mean_squared_error/neg_mean_absolute_error/accuracy/neg_log_loss/roc_auc/f1/recall"
		for name, model in models:
			folds = KFold(n_splits = 5, random_state = 4)
			cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)
			results.append(cv_results)
			names.append(name)
			print("{0} - {1} ({2})".format(name, cv_results.mean(), cv_results.std()))
		
		# boxplot algorithm comparison
		fig = plt.figure()
		fig.suptitle('Algorithm Comparison')
		ax = fig.add_subplot(111)
		plt.boxplot(results)
		ax.set_xticklabels(names)
		plt.show()

	## pre-processing

		# scale
		from sklearn.preprocessing import scale
		X_scaled = scale(X)

		# MinMaxScaler
		from sklearn.preprocessing import MinMaxScaler
		min_max_scaler = MinMaxScaler(feature_range=(0, 1)).fit(X_train)
		X_train_scaled = min_max_scaler.transform(X_train)
		X_test_scaled = min_max_scaler.transform(X_test)

		# StandardScaler (0-mean, 1-standard deviation)
		from sklearn.preprocessing import StandardScaler
		standard_scaler = StandardScaler().fit(X_train)
		X_train_scaled = standard_scaler.transform(X_train)
		X_test_scaled = standard_scaler.transform(X_test)

		# Normalizer - useful when data is sparse with lots of zeros
		from sklearn.preprocessing import Normalizer
		normalizer = Normalizer().fit(X_train)
		X_train_scaled = normalizer.transform(X_train)
		X_test_scaled = normalizer.transform(X_test)

		# binarizer - convert all values above the threshold to 1 and all values below the threshold to 0
		from sklearn.preprocessing import Binarizer
		binarizer = Binarizer(threshold=0.5).fit_transform(X)

		# pipeline - scale and model
		from sklearn.preprocessing import StandardScaler   # google - preprocessing types in scikit-learn
		steps = [("scaler", StandardScaler()),
				 ("model_name", model)]
		pipeline = Pipeline(steps)
		pipeline.fit(X_train, y_train)                     # pipeline can be passed to GridSearchCV() instead of directly fitting
		y_pred = pipeline.predict(X_test)

		# handling missing values

		from sklearn.preprocessing import Imputer
		imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
		imputer.fit(X)
		X = imputer.transform(X)                           # return imputed dataframe

		# pipeline - imputing and modelling in one go
		from sklearn.pipeline import Pipeline              # alternative - import make_pipeline
		imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
		steps = [("imputer", imputer),
				 ("model_name", model)]
		pipeline = Pipeline(steps)
		pipeline.fit(X_train, y_train)
		y_pred = pipeline.predict(X_test)

	## NLP

		# CountVectorizer() creates tokens, builds vocabulary and counts frequency of each word
		from sklearn.feature_extraction.text import CountVectorizer           # for large text datasets, use HashingVectorizer
		word_vector = CountVectorizer(token_pattern = "", ngram_range = (1))  # split tokens based on given pattern
		word_vector.fit(df.text_column)                    # learn vocabulary from the given Series
		word_vector.transform(df.text_column)              # create document term matrix
		word_vector.fit_transform(df.text_column)          # fit and transform in one go

		# combining text and numeric pipelines for them to be ready for final common pipeline
		from sklearn.preprocessing import FunctionTransformer
		from sklearn.feature_selection import chi2, SelectKBest
		from sklearn.preprocessing import SparseInteractions

		get_text_data = FunctionTransformer(lambda x: x.text_column, validate = False)  # validate checks for NaN values
		get_numeric_data = FunctionTransformer(lambda x: x.numeric_column, validate = False)

		numeric_pipeline = Pipeline([
						       ("selector": get_numeric_data),
						       ("imputer": Imputer())
						   ])

		text_pipeline = Pipeline([
							("selector": get_text_data),
							("vectorizer": CountVectorizer()),
							("dim_red": SelectKBest(chi2, 100))  # select 100 best features using chi-squared test
						])

		from sklearn.pipeline import FeatureUnion
		union = FeatureUnion([
					("numeric": numeric_pipeline),
					("text": text_pipeline)
				])

		final_pipeline = Pipeline([
						 	("union": union),
						 	('int', SparseInteractions(degree=2)),
						 	("scale": MaxAbsScaler()),
						 	("model": model)
						 ])

		final_pipeline.fit(X_train, y_train)

	## unsupervised learning

		# k-means clustering
		from sklearn.cluster import KMeans
		model = KMeans(n_clusters = )
		model.fit(X)
		model.inertia_
		model.cluster_centers_
		clusters = model.predict(X)
		new_labels = model.predict(X_new)

		# hierarchical clustering
		# In complete linkage, the distance between clusters is the distance between the furthest points of the clusters.
		# In single linkage, the distance between clusters is the distance between the closest points of the clusters.
		
		from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
		mergings = linkage(X, method = "complete")
		clusters = fcluster(mergings, 15, criterion = "distance")  # dendrogram height = 15
		pd.DataFrame({"labels": y, "cluster": clusters})           # cluster number starts from index 1

		dendrogram(mergings, labels = y, leaf_rotation = , leaf_font_size = )  # visualize dendrogram
		plt.show()

		# t-SNE - visualize n-dimensional data into 2-dimensional
		from sklearn.manifold import TSNE
		model = TSNE(learning_rate = 100)  # Optimal choice [50,200]. Points should appear in clusters i.e. not intermingled.
		transformed = model.fit_transform(X)
		plt.scatter(transformed.iloc[:, 0], transformed.iloc[:, 1], c = y)
		plt.show()

		# PCA
		from sklearn.decomposition import PCA
		model = PCA(n_components = )       # First do model = PCA(). n_components is known after plotting the bar plot shown below.
		df_pca = model.fit_transform(df)   # Or use fit() and transform() separately. Also, it is better to scale data before applying PCA.
		model.components_
		features = range(model.n_components_)
		plt.bar(features, model.explained_variance_)
		plt.xticks(features)
		plt.show()

		# pca for sparse matrices like word frequencies. google - TfidfVectorizer
		from sklearn.decomposition import TruncatedSVD
		model = TruncatedSVD(n_components = )
		df_pca = model.fit_transform(sparse_df)  # sparse_df is scipy.sparse.csr_matrix instead of numpy array.

		# non negative matrix factorization - interpretable unlike pca
		from sklearn.decomposition import NMF
		model = NMF(n_components = n)  # always specify n_components unlike PCA()
		nmf_features = model.fit_transform(df)   # data should have non-negative entries only
		model.components_

		# recommendor system using NMF
		nmf_features = normalize(nmf_features)
		df = pd.DataFrame(nmf_features, index = "article_names")
		current_article = df.loc("article_name")
		similarities = df.dot(current_article)  # cosine similarity. the more the similar
		print(similarities.nlargest())          # see similarities between current article and other articles. Recommend the most similar article.

==============================================================================================================================

### keras -- deep learning

		X_train = df.drop(["target_column"], axis = 1).as_matrix()   # size - [n_rows, n_cols]

		# one hot encoding the outcome variable
		from keras.utils import to_categorical
		y_train = to_categorical(df.target_column)

		n_cols = X_train.shape[1]
		num_classes = y_train.shape[1]

		# Steps in keras modelling
		1. Create architecture
		2. Compile model
		3. Fit model
		4. Predict using model

	## 1. Create architecture

		from keras.models import Sequential
		from keras.layers import Dense
		from keras.layers import Dropout

		# Set up the model
		model = Sequential()

		# Add first hidden layer
		model.add(Dense(units                 =   n_neurons,      # number of neurons in the layer     
						activation            =   "",             # activation function. None means no activation
						input_shape           =   (n_cols,),      # mention this for the first hidden layer only
						use_bias              =   True,           # whether the layer uses bias vector or not
						kernel_initializer    =   ,               # weights initializer
						bias_initializer      =   ,               # bias initializer
						kernel_regularizer    =   ,               # use regularization on weight matrix
						bias_regularizer      =   ,               # use regularization on bias vector
						activity_regularizer  =   ,               # use regularization on output of activation
						kernel_constraint     =   ,               # contraint function, generally used when dropout is used
						bias_constraint       =                   # contraint function, generally used when dropout is used
		))

		# add drouput to the previous layer
		model.add(Dropout(rate = 0.2, seed = 4))

		# Add second hidden layer
		model.add(Dense(n_neurons, activation = ""))

		# Add the output layer
		model.add(Dense(1))                                       # for regression
		model.add(Dense(num_classes, activation = "softmax"))     # for classification, one neuron for each class and a separate activation function

		# visualize model
		from keras.utils import plot_model
		plot_model(model, to_file = "model.png", show_shapes = False, show_layer_names = True, rankdir = "TB/LR")

	## 2. Compile the model

		from keras.optimizers import SGD
		sgd = SGD(lr = 0.01, momentum = 0.0, decay = 0.0, nesterov = False)
		model.compile(optimizer  =   "adam"/sgd,
					  loss       =   "categorical_crossentropy",
					  metrics    =   ["accuracy"]
		)

	## 3. Fit the model

		# stop when there is no improvement in validation set score
		from keras.callbacks import EarlyStopping
		early_stopping_monitor = EarlyStopping(patience = 2)    # stop when there is no improvement for 3 consecutive epochs
		
		# checkpoint model - save weights
		from keras.callbacks import ModelCheckpoint
		filepath = "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"  # multiple checkpoints as a result of different file names
		filepath = "weights-improvement.hdf5"                            # checkpoints will be overwritten to this file
		checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose = 1, save_best_only = True, mode = 'max')
		callbacks_list = [early_stopping_monitor, checkpoint]
		
		training = model.fit(x                =   X_train,
							 y                =   y_train,
				  			 batch_size       =   64,
				  			 epochs           =   10,
				  			 verbose          =   1,                     # 0, 1 or 2
							 callbacks        =   callbacks_list,        # google keras callbacks techniques for advanced deep learning
				  			 validation_split =   0.25,
				  			 validation_data  =   (X_test, y_test),      # pass validation_split or validation_data
				  			 shuffle          =   True,                  # shuffle data before each epoch
				  			 class_weight     =   None                   # assign weight in case of unbalanced class
		)
		model.summary()
		training.history.keys()

		# load weights
		model.load_weights("weights.best.hdf5")                 # load weights only after model is fitted
		
	## 4. Evaluate/Predict using the model

		# evaluate model performance on test set
		scores = model.evaluate(X_test, y_test)
		print(model.metrics_names)
		
		# predict values on test set
		y_pred = model.predict(X_test)

		# visualise training history for accuracy
		plt.plot(training.history['acc'])
		plt.plot(training.history['val_acc'])
		plt.title('model accuracy')
		plt.ylabel('accuracy')
		plt.xlabel('epoch')
		plt.legend(['train', 'test'], loc="upper left")
		plt.show()
		# visualise training history for loss
		plt.plot(training.history['loss'])
		plt.plot(training.history['val_loss'])
		plt.title('model loss')
		plt.ylabel('loss')
		plt.xlabel('epoch')
		plt.legend(['train', 'test'], loc="upper right")
		plt.show()

	## save and load model

		# save model
		model.save("my_model.h5")
		
		# load model
		from keras.models import load_model
		model = load_model("my_model.h5")

	## keras and scikit-learn

		# 1. Build function that defines, compiles and returns a keras model
		def create_model(optimizer = "", loss = "", weight_initializer = ):
			# create model
			model = Sequential()
			model.add(Dense(n_nodes, activation = "", input_shape = (n_cols,), kernel_initializer = weight_initializer))
			model.add(Dense(n_nodes, activation = ""))
			model.add(Dense(2, activation = "softmax"))
			# Compile model
			model.compile(loss = loss, optimizer = optimizer, metrics = [""])
			return model

		# 2. Create scikit-learn compatible model using keras wrapper
		from keras.wrappers.scikit_learn import KerasClassifier
		from keras.wrappers.scikit_learn import KerasRegressor
		model = KerasClassifier(build_fn = create_model, epochs = , batch_size = 10, verbose = 1)
		model = KerasRegressor(build_fn = create_model, epochs = , batch_size = 10, verbose = 1)

		# 3. Use model as a native scikit-learn model
		cv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = score)

		OR

		optimizers = ['rmsprop', 'adam']
		weight_initializers = ['glorot_uniform', 'normal', 'uniform']
		epochs = [50, 100, 150]
		batches = [32, 64, 128]
		params = {"optimizer"           : optimizers,
				  "epochs"              : epochs,
				  "batch_size"          : batches,
				  "weight_initializer"  : weight_initializers
		}
		model_cv = GridSearchCV(estimator = model, param_grid = params)
		model_cv.fit(X_train, y_train)
		model_cv.cv_results_
		model_cv.best_score_
		model_cv.best_params_

	## learning rate schedules

		# time-based decay - drop learning rate after each epoch
		epochs = 10
		learning_rate = 0.1
		decay_rate = learning_rate / epochs
		momentum = 0.9
		sgd = SGD(lr = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)

		# drop-based decay - drop learning rate after each specified number of epochs
		from keras.callbacks import LearningRateScheduler
		def step_decay(epoch):
			"""
			half the learning rate every 10 epochs
			input: epoch
			output: learning rate
			"""
			initial_lr = 0.1                     # initial learning rate
			drop = 0.5                           # drop learning rate by half
			epochs_drop = 10.0                   # number of epochs after which to drop learning rate
			lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))
			return lr

		lr_scheduler = LearningRateScheduler(step_decay)
		callbacks_list = [lr_scheduler]

	# CNN

		from keras.models import Sequential
		from keras.layers import Dense
		from keras.layers import Dropout
		from keras.layers import Flatten
		from keras.layers import Conv2D
		from keras.layers import MaxPooling2D
		from keras.utils import np_utils

		# reshape data to work with CNN - [n_rows, height, width, channels]
		X_train = X_train.reshape(n_rows, height, width, channels).astype('float32')
		X_test = X_test.reshape(n_rows, height, width, channels).astype('float32')
		
		# model architecture
		model = Sequential()
		model.add(Conv2D(filters               =   32,
						 kernel_size           =   (5, 5),
						 input_shape           =   (height, width, channels),
						 strides               =   1,
						 padding               =   "valid/same",       # valid - no padding, same - keeps same dimensions
						 activation            =   "relu",
						 use_bias              =   True,               # whether the layer uses bias vector or not
						 kernel_initializer    =   ,                   # weights initializer
						 bias_initializer      =   ,                   # bias initializer
						 kernel_regularizer    =   ,                   # use regularization on weight matrix
						 bias_regularizer      =   ,                   # use regularization on bias vector
						 activity_regularizer  =   ,                   # use regularization on output of activation
						 kernel_constraint     =   ,                   # contraint function, generally used when dropout is used
						 bias_constraint       =                       # contraint function, generally used when dropout is used
		))
		model.add(MaxPooling2D(pool_size=(2, 2)))
		model.add(Dropout(0.2))                                        # drop 20% neurons
		model.add(Flatten())
		model.add(Dense(128, activation='relu'))
		model.add(Dense(num_classes, activation='softmax'))

	# RNN

		# embeddings - for text data
		vocab_size        = 5000
		max_review_length = 400
		from keras.preprocessing import sequence
		X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='pre', truncating='pre', value=0)
		X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='pre', truncating='pre', value=0)
		model = Sequential()
		model.add(Embedding(input_dim          =   vocab_size,         # vocabulary size
							output_dim         =   32,                 # dimension of embedding
							input_length       =   max_review_length   # length of review
		))
		# add subsequent layers here like (Flatten() and Dense()) or LSTM()
		""" 
		The model will take as input an integer matrix of size (batch, input_length).
		The largest integer (i.e. word index) in the input should be no larger than 4999 (vocabulary size-1).
		Now model.output_shape == (None, max_review_length, 32), where None is the batch dimension.
		Refer documentation for a small comprehensive code - very nice!
		"""

		"""
		In time series prediction, scale full dataset(X and y), divide into train and test, fit the model and predict.
		After prediction, undo the scaling by using min_max_scaler.inverse_transform(y_pred) for y_train, y_pred_train, y_test and y_ pred_test.
		Then compare y_train with y_pred_train and y_test with y_pred_test.
		"""

		# reshape data - [n_rows, time steps, n_cols] or [n_rows, n_cols, time steps] for time series data
		X_train = X_train.reshape((n_rows, time_steps, n_cols))
		X_test = X_test.reshape((n_rows, time_steps, n_cols))

		# simple LSTM
		from keras.models import Sequential
		from keras.layers import Dense
		from keras.layers import LSTM

		model = Sequential()
		model.add(LSTM(units                   =   n_cells,            # number of LSTM cells aka neurons
					   activation              =   "",
					   recurrent_activation    =   "",                 # activation to use for the recurrent step
					   input_shape             =   time_steps,         # not sure if this is required if LSTM is first layer
					   dropout                 =   0.2,                # dropout at input for each cell of LSTM
					   recurrent_dropout       =   0.1                 # dropout at output for each cell of LSTM
		))
		model.add(Dense(1))

		OR
		
		model = Sequential()
		model.add(Embedding(input_dim = , output_dim = , input_length = ))
		model.add(LSTM(units = n_cells, activation = ""))
		model.add(Dense(1))

		# stateful LSTM
		batch_input_shape = (batch_size, time_steps, n_cols)
		model.add(LSTM(units                   =   n_cells,            # number of LSTM cells aka neurons
					   activation              =   ,
					   batch_input_shape       =   batch_input_shape,  # add this to each LSTM layer in case of stateful LSTM
					   stateful                =   True                # statefulness set to True
		))
		model.add(Dense(1))

		for i in range(n_epochs):                                      # create manual epochs. Can't set epochs because can't pass states to next epoch.
			model.fit(x             =   X_train,
					  y             =   y_train,
					  batch_size    =   64,
					  epochs        =   1,                             # epochs should be 1 since manual epochs in process
					  verbose       =   1,
					  shuffle       =   False                          # don't shuffle data before each epoch
			)
			model.reset_states()                                       # reset state after each epoch

		y_pred = model.predict(X_train, batch_size = 64)               # prediction should be in batches
		model.reset_states()                                           # reset state after predicting on a specific dataset
		y_pred = model.predict(X_test, batch_size = 64)                # prediction should be in batches
		
		model.evaluate(X_test, y_test, batch_size = 64)
		model.reset_states()

		# stacked LSTMs
		"""
		LSTMs can be stacked. The last LSTM layer has to have return_sequences = True
		"""
		model.add(LSTM(units = n_cells, batch_input_shape = batch_input_shape, stateful = True, return_sequences = True))
		model.add(LSTM(units = n_cells, batch_input_shape = batch_input_shape, stateful = True))
		model.add(Dense(1))

		# LSTM and CNN
		from keras.models import Sequential
		from keras.layers import Dense
		from keras.layers import LSTM
		from keras.layers.convolutional import Convolution1D
		from keras.layers.convolutional import MaxPooling1D
		from keras.layers.embeddings import Embedding

		model = Sequential()
		model.add(Embedding(input_dim = vocab_size, output_dim = 32, input_length = max_review_length))
		model.add(Convolution1D(filters = 32, kernel_size = 3))  # same parameters as Convolution2D
		model.add(MaxPooling1D(pool_size = 2)
		model.add(LSTM(100))
		model.add(Dense(1))

==============================================================================================================================

### NLP

		# preprocessing - lowercase -> stem -> tokenize -> remove stop words -> remove punctuation -> remove special characters
		import re
		pattern = r""
		re.match(pattern, string)                          # look for the pattern from the start of the string
		match.start(); match.end()
		re.search(pattern, string)                         # look for the pattern in the whole string and returns the first occurence
		re.findall(pattern, string)                        # look for and returns all the occurences of patterns present in the string
		re.split(pattern, string)                          # split the string at given pattern
		re.sub(pattern, repl, string)                      # replace first occurence of pattern by repl in the string

		from nltk.tokenize import word_tokenize
		word_tokenize(string, pattern)                     # tokenize string into words

		from nltk.tokenize import sent_tokenize
		sent_tokenize(string, pattern)                     # tokenize string into sentences

		from nltk.tokenize import regexp_tokenize
		regexp_tokenize(string, pattern)                   # tokenize string based on pattern

		from nltk.tokenize import TweetTokenizer
		tokenizer = TweetTokenizer()
		tokenizer.tokenize(string)

		# plot word count
		words = [word_tokenize(line) for line in book]     # return list of words of the entire book
		word_lengths = [len(word) for word in words]       # return word length of each word
		plt.hist(word_lengths)

		# bag of words
		from collections import Counter
		word_frequencies = Counter(word_tokenize(string))  # return the word along with their count
		print(word_frequencies.most_common(5))             # return the top 5 words occuring in the string

		# text preprocessing
		from nltk.corpus import stopwords
		alpha_only = [t for t in word_tokenize(text.lower()) if t.isalpha()]    # return lowercase alphabetic token
		no_stops = [t for t in tokens if t not in stopwords.words("english")]   # remove stop words

		from nltk.stem import WordNetLemmatizer
		wordnet_lemmatizer = WordNetLemmatizer()
		lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

	## gensim - create bag of words

		documents = ["doc1", "doc2", "doc3"]
		articles = [word_tokenize(doc.lower() for doc in documents)]
		
		from gensim.corpora.dictionary import Dictionary
		dictionary = Dictionary(articles)                  # articles is preprocessed and tokenized
		
		# get token id
		dictionary.token2id                                # return all tokens with their respective ids
		token_id = dictionary.token2id,get("cars")         # return token id for "cars"

		# get token using token id
		dictionary.get(token_id)
		
		# create corpus
		corpus = [dictionary.doc2bow(article) for article in articles]
		corpus[0][:10]                                     # print first 10 word ids with their frequency from first document
		doc = corpus[0]                                    # doc has word ids with their frequency for first document

		# tf-idf
		from gensim.models.tfidfmodel import TfidfModel
		tfidf = TfidfModel(corpus)                         # train tfidf model on corpus
		tfidf_weights = tfidf[doc]                         # get work_ids and their tfidf weight of a specific document
		sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)  # sort according to tfidf weights

		for term_id, weight in sorted_tfidf_weights[:5]:
		    print(dictionary.get(term_id), weight)         # top five words with their tf-idf value

		# named entity recognotition (NER) using spacy
		import spacy
		nlp = spacy.load("en")                             # load english corpus
		npl.entity                                         # 
		doc = nlp("sample text")                           # NER
		doc.ents                                           # look at entities
		for ent in doc.ents:
    		print(ent.text, ent.label_)                    # look at entities and their labels

==============================================================================================================================
