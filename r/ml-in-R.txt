### Linear Regression

	# check r-squared value. It has to be maximized for it be a good model. also check significance of variables. Remove
	# insignificant variables. Two tied insignificant vars can be removed by checking pr(>|t|) value. remove
	# insignificant vars with high pr(>|t|) value. check sign of estimate values of independent vars in summary(model)
	# and see if that makes sense. check for multicollinearity among independent vars(>0.7) and remove accordingly one of the two.

	# model
		library(Metrics)

		linear.model <- lm(target ~ ., data = train)
		linear.predict <- predict(linear.model, cv)

	# calc RMSE and R-squared

		rmse <- function(predicted, actual)
		{
    		sum.of.squared.error <- sum((predicted - actual)^2)
    		root.mean.squared.error <- sqrt(sum.of.squared.error/length(actual))
    		return(root.mean.squared.error)
		}

		rsquared <- function(predicted, cv.label, train.label)
		{
    		sse <- sum((predicted - cv.label)^2)
			sst <- sum((mean(train.label) - cv.label)^2)
			r.squared <- 1-(sse/sst)
			return(r.squared)
		}

		error <- rmse(cv$target, linear.predict)

	# ROCR and AUC (Area Under Curve)
		
		library(ROCR)
		
		linear.predict <- predict(linear.model, cv)
		linear.ROCR <- prediction(linear.predict, cv$target)
		linear.auc <- as.numeric(performance(linear.ROCR,"auc")@y.values)

	# step model

		step.model <- step(linear.model)

	## Variable Selection

		# 1. Select important variables using stepAIC()
		linear.model <- stepAIC(linear.model, direction = "both/backward/forward")

		# 2. Remove one variable which has high vif (more than 2, 3 or 4) and high p-value (more than 0.01, 0.05 or 0.1).
		vif(linear.model)        # check vif values
		summary(linear.model)    # check p-values

		# 3. Rebuild model after removing one variable at a time, using the above step
		linear.model <- lm(formula = , data = train)

		# Step 2 and 3 are repeated until all the variables have a required vif (say, less than 3), and a required p-value (say, less than 0.001)
		# Tip: When vif values are low enough, you can skip checking vif values. Only check the p-values.

==============================================================================================================================

### Logistic Regression
	
	## Binomial

		glm.model <- glm(target ~ ., data = train, family = "binomial", control = list(maxit = 50))  # OR family = multinomial()
		glm.predict <- predict(glm.model, cv, type = "response")
		table(cv$target, glm.predict>0.5)
		error <- ce(cv$target, glm.predict)
	
	## Multinomial

		library ("nnet")

		multinom.model <- multinom(target ~ ., data = train)
		summary(multinom.model)
		multinom.predict <- predict(multinom.model, cv, "probs")

	## ROCR and AUC(area under curve)
	
		library(ROCR)
		library(pROC)

		roc_curve <- roc(cv$target, algo.predict, plot=TRUE, auc=TRUE, grid=TRUE, col="blue")
		plot(roc_curve)
		lines(roc_curve2)  # add roc_curve2 to roc_curve plot

		glm.predict <- predict(model, cv)
		glm.ROCR <- prediction(glm.predict, cv$target)
		glm.AUC <- as.numeric(performance(glm.ROCR,"auc")@y.values)

		glm.prediction <- prediction(abs(glm.predict), cv$target)
		glm.performance <- performance(glm.prediction,"tpr", "fpr")
		plot(glm.performance)

	## Lift chart

		glm.prediction <- prediction(abs(glm.predict), cv$target)
		glm.performance <- performance(glm.prediction, "lift", "rpp")  # google ROCR performance functions for more charts
		plot(glm.performance)

==============================================================================================================================

### K-Nearest Neighbor

		library("class")

		predictors <- names(train)[names(train) != "target"]
		knn.trainX <- train[, predictors]
		knn.cvX <- cv[, predictors]
		knn.trainY <- train$target
		knn.cvY <- cv$target
		
		# make sure all variables are numeric
		set.seed(1)
		knn.model <- knn(train = knn.trainX, test = knn.cvX, cl = knn.trainY, k = under.root.observations)
		confusionMatrix(knn.model, knn.cvY)

==============================================================================================================================

### Regularized Regression

		library("glmnet")
		
		# input numerical data only
		predictors <- names(train)[names(train) != "target"]
		trainX <- data.matrix(train[ ,predictors])
		trainY <- train$target
		cvX <- data.matrix(cv[ ,predictors])
		cvY <- cv$target

		cv.glmnet.model <- cv.glmnet(trainX, trainY,
									 type.measure = "deviance/mse/mae/class/auc",
									 family = "binomial",  # remove if not a classification problem
									 nfolds = 10, nlambda = 100)
		cv.glmnet.model
		plot(cv.glmnet.model)
		
		glmnet.model <- glmnet(trainX, trainY,
								family = "gaussian","binomial","multinomial",
								alpha = ,  # 0 - ridge, 1 - lasso, 0.5 - elnet
								lambda = cv.glmnet.model$lambda.1se  # or cv.glmnet.model$lambda.min
								)

		glmnet.predict <- predict(glmnet.model, cvX, s = cv.glmnet.model$lambda.1se)  # or cv.glmnet.model$lambda.min
		table(cv$target, glmnet.predict)

==============================================================================================================================

### SVM

		library("e1071")

		svm.model <- svm(target ~ ., data = train, kernel = "radial", cost = 1, gamma = 0.1)
		svm.predict <- predict(svm.model, cv)
		plot(svm.model, train)
		table(cv$target, svm.predict)

==============================================================================================================================

### Decision Trees
	
	# for classification use method = "class" in rpart and type = "class" in predict
		library("rpart")
		library("rpart.plot")
		
	# model
		
		tree.model <- rpart(target ~ ., data = train, method = "class/anova", parms = list(split = "gini/information"),
							control = rpart.control(cp = , minsplit = , minbucket = ))
		prp(tree.model) 
		tree.predict <- predict(tree.model, cv, type = "class") # no need to specify "class" for regression
		table(cv$target, tree.predict)

	# ROCR and AUC(area under curve)
		
		tree.predict <- predict(tree.model, cv) # type = "class" not to be included here
		tree.ROCR <- prediction(tree.predict[,2], cv$target)
		tree.auc <- as.numeric(performance(tree.ROCR,"auc")@y.values)

==============================================================================================================================

### Random Forest
		
		library("randomForest")
		
	# model

		set.seed(10)
		rf.model <- randomForest(target ~ ., data = train,
								 importance = TRUE,
								 ntree = 1000,
								 mtry = root.of.variables,
								 do.trace = TRUE,
								 na.action = na.omit
								)
		varImpPlot(rf.model)
		rf.predict <- predict(rf.model, cv)
		table(cv$target, rf.predict)
		
	# party-cforest

		library("party")

		cforest.model = cforest(target ~ ., data = train, controls = cforest_unbiased(ntree = 1000, mtry = root.of.variables))
		cforest.prediction = predict(cforest.model, cv, OOB = TRUE, type = "response")
		table(cv$target, cforest.prediction)

==============================================================================================================================

### GBM

		library("gbm")

		gbm.model <- gbm(target ~ ., # if target doesn't work set to as.integer(target) after converting to numeric manually.
						 distribution = c("bernoulli","multinomial","gaussian"), # multinomial more robust even in binomial case
						 data = train,
						 n.trees = 2000,
						 interaction.depth = 1,
						 n.minobsinnode = 10,
						 shrinkage = 0.001,
						 train.fraction = 1.0,
						 keep.data = TRUE,
						 verbose = TRUE
						 )

		gbm.perf(gbm.model)

		gbm.predict <- predict(gbm.model, cv, n.trees = gbm.perf(gbm.model, plot.it = F), type = "response")
		gbm.predict <- apply(gbm.predict, 1, which.max) # choose class with maximum probability

		table(gbm.predict, cv$target)

==============================================================================================================================

### H2O deep-learning

		library(h2o)

		localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '2g')
		
		names(train) <- NULL # optional. so that the algorithm does not take column name as a separate level in the target variable
		h2o.train <- as.h2o(train)
		h2o.cv <- as.h2o(cv)

		h2o.model <- h2o.deeplearning(x = setdiff(names(train), c("target","id")), # predictors names or indices
							y = "target", # label. check names(h2o.train) to see target variable name
							training_frame = h2o.train, # data to train
							activation = "TanhWithDropout", # or 'Tanh'
							standardize = TRUE,
							input_dropout_ratio = 0.2, # % of inputs dropout
							hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
							balance_classes = TRUE, # for classification tasks only
							hidden = c(50,50,50), # two hidden layers with 100 nodes each
							epochs = 100, # number of iterations on data
							seed = 1, # for reproducability
							variable_importances = T,
							nfolds = 5)

		h2o.predictions <- as.data.frame(h2o.predict(h2o.model, h2o.cv))
		confusionMatrix(h2o.predictions$predict, cv$target) # remove first row of h2o.predictions if length does not match
		h2o.varimp_plot(h2o.model,num_of_features = 20)
		h2o.performance(h2o.model,xval = T)

==============================================================================================================================

### xgboost
		
		library(xgboost)

		# XGBOOST caret

		xgb.grid <- expand.grid(
		    eta = c(0.3,0.2,0.1,0.01),  # learning rate. [default=0.3][range: (0,1)]
			max_depth = c(6, 10),       # depth of tree. [default=6][range: (0,Inf)]
			nrounds = 100,              # iterations. [default=100]
			colsample_bytree = 1,       # proportion of features supplied to a tree. [default=1][range: (0,1)]
		    subsample = 1,              # proportion of samples supplied to a tree. [default=1][range: (0,1)]
		    min_child_weight = 1,       # minimum number of instances required in a child node. [default=1][range:(0,Inf)]
		    gamma = c(0,5)              # increased gamma to increase level of regularization. [default=0][range: (0,Inf)]
		    lambda = 0,                 # L2 regularization(Ridge). [default=0]
		    alpha = 1                   # L1 regularization(Lasso). more useful on high dimensional data sets. [default=1]
		    )

		xgb.control <- trainControl(
		    method="cv",
		    number = 5,
		    verboseIter = TRUE,
		    returnData = FALSE,
		    returnResamp = "all",
		    allowParallel = TRUE
			)

		xgb.predictors <- as.matrix(train[, !(names(train) %in% c("target","id"))])
		xgb.label <- train$target

		xgb.model <- train(x = xgb.predictors,
		    y = xgb.label,
		    trControl = xgb.control,
		    tuneGrid = xgb.grid,
		    method="xgbTree" # "xgbLinear"
		)

		xgb.predict <- predict(xgb.model, data.matrix(cv))
		confusionMatrix(xgb.predict, cv$target)
		important.features <- varImp(xgb.model)
		plot(important.features, 20)

		# XGBOOST manual

		data.train <- xgb.DMatrix(data = train$data, label = train$label)
		data.cv <- xgb.DMatrix(data = cv$data, label = cv$label)

		watchlist <- list(train  = data.train, test = data.cv)

		parameters <- list(
      # General Parameters
          booster            = "gbtree",          # default = "gbtree"
          silent             = 0,                 # default = 0
      # Booster Parameters
          eta                = 0.3,               # default = 0.3, range: [0,1]
          gamma              = 0,                 # default = 0,   range: [0,∞]
          max_depth          = 6,                 # default = 6,   range: [1,∞]
          min_child_weight   = 1,                 # default = 1,   range: [0,∞]
          subsample          = 1,                 # default = 1,   range: (0,1]
          colsample_bytree   = 1,                 # default = 1,   range: (0,1]
          colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
          lambda             = 1,                 # default = 1
          alpha              = 0,                 # default = 0
      # Task Parameters
          objective          = "reg:logistic",    # default = "reg:linear"
          eval_metric        = "error",
          num_classes        = 2,                 # number of classes in case of multi class classification
          seed               = 1234				  # reproducability seed
        )

		xgb.model <- xgb.train(parameters, data.train, nrounds = 20, watchlist)
		xgb.predict <- predict(xgb.model, data.cv)
		confusionMatrix(xgb.predict, cv$target)

		predictor.names <- dimnames(data.matrix(train[, !(names(train) %in% c("target"))]))[[2]]
		importance_matrix <- xgb.importance(predictor.names, model = xgb.model)
		xgb.plot.importance(importance_matrix[1:20,])  # plot top 20 important variables

==============================================================================================================================

### caret

		library("caret")
		
		f <- as.formula(paste("target ~", paste(names(train)[!names(train) %in% c("target")], collapse = " + ")))

		algo.control = trainControl(method = "cv", 
									number = 5,
									classProbs = T,
									summaryFunction = defaultSummary,  # twoClassSummary for binary classification
									verboseIter = T,
									allowParallel = T
									)

		algo.grid = expand.grid(model_specific_parameters)

		algo.model <- train(target ~ .,
							data = train,
							method = "",
							preProcess = c("center","scale"),  # zv/nzv, medianImpute/knnImpute, center, scale, pca - imp for linear models
							metric = "",
							trControl = algo.control,
							tuneGrid = algo.grid
							)

		trellis.par.set(caretTheme())
		plot(algo.model, metric = "Accuracy")  # look at ?plot.train

		algo.predict <- predict.train(algo.model, cv, type = "raw/prob")
		confusionMatrix(algo.predict, cv$target, mode = "prec_recall", positive = "class_name")

		imp <- varImp(algo.model)
		plot(imp, top = 20)

		# compare models
		model_list <- list(RF = rf.model,
						   GBM = gbm.model,
						   )
		resamps <- resamples(model_list)
		resamps
		summary(resamps)

		bwplot(resamps, layout = c(3, 1))
		dotplot(resamps, metric = "ROC")
		densityplot(resamps, metric = "ROC")
		xyplot(resamps, metric = "ROC")

==============================================================================================================================

### K-fold cross validation

		# Set number of folds
		k <- 10

		# Randomly shuffle the data
		data.frame <- data.frame[sample(nrow(data.frame)), ]

		# Create K equally size folds
		folds <- cut(seq(1, nrow(data.frame)), breaks = k, labels = FALSE)

		accuracy <- vector(mode = "integer", length = k)
		x <- 0

		# Perform K-fold cross validation
		for(i in 1:k){
		    if(i == 1) cat("Fold", "\t", "Accuracy", "\n")

		    x <- x + 1
		    #Segment your data by fold using the which() function 
		    cv.indices <- which(folds == i, arr.ind=TRUE)
			train <- data.frame[-cv.indices, ]
			cv <- data.frame[cv.indices, ]

			# model

			# calculate accuracy for current fold
			accurate.predictions <- 0
			confusion.matrix <- as.matrix(table(cv$target, predictions))
			
			for(i in 1:nrow(confusion.matrix)){
				accurate.predictions <- accurate.predictions + confusion.matrix[i,i]
			}

			accuracy[x] <- accurate.predictions/nrow(cv)
			cat(x, "\t", round(accuracy[x], 4), "\n")
			if(x == k)
			{
				cat("Mean Accuracy", "\n")
				cat(round(mean(accuracy), 4))
			}
		}

==============================================================================================================================
### PCA

		pca <- prcomp(data_frame, center = TRUE, scale = TRUE)
		summary(pca)
		plot(pca, type="l")
		pca.data <- pca$x

==============================================================================================================================
### PCR

		library(pls)
		pcr.model <- pcr(target ~ ., data = train, scale =T, center = T, validation = "CV")
		pcr.predict <- predict(pcr.model, cv)
		rmse(pcr.predict, cv$target)

==============================================================================================================================
### Clustering

	data_frame -> as.matrix -> as.vector -> clustering -> dim(vector) -> image.output
	data_frame -> as.matrix -> as.vector -> test

	# after clustering we can subset the data according to the clusters(for hierarchical especially) to study number of rows in each cluster.
	# cluster1 <- subset(train , cluster == nth.cluster.number)

	# Hierarchical

		distances <- dist(movie[2:20] OR vector, method = "euclidean")
		hcluster <- hclust(distances, method = "ward.D")
		plot(hcluster)
		hclusterGroups <- cutree(cluster, k = no.of.clusters)
		hclusterGroups[index.of.var.to.see.which.cluster.it.belongs.to]

	# K-Means

		library("flesclust")
		set.seed(1)
		kcluster <- kmeans(vector OR data_frame, centers = no.of.centroids, iter.max = no.of.max.iterations)
		str(kcluster)
		
		# rest of this for cluster-then-predict
		# in cluster-then-predict problems, (remove target var->normalize(optional)->cluster->kcca)=>build "k" train and test sets using subset from original train according to clusters.
		# example -> newTrain1 = subset(originalTrain, kclusterPredictTrain == 1), newTrain2 = subset(originalTrain, kclusterPredictTest == 2)
		
		kclusterKcca <- as.kcca(kcluster, originalDataFrame OR originalVector)
		kcluster.predict.train <- predict(kclusterKcca)
		kcluster.predict.test <- predict(kclusterKcca, newdata = test.data.as.vector)

		#easy way to split according to clusters in k-means
		
		KmeansCluster = split(data_frame, kcluster$cluster) # KmeansCluster[[1]] and so on to access 1st and other successive clusters

==============================================================================================================================

### Ensembling

	# 1. Split data into train, cv and test. Create multiple models using different ml algorithms on train set.
	# 2. Make a data frame ensemble_train which includes predictions of these multiple models (on cv) in each column along with the cv target variable.
	# ex: suppose model1 and model2 predict pred1_cv and pred2_cv on cv set then this ensemble_train contains pred1_cv,pred2_cv,cv_targetVariable.

	# 3. Now make predictions on test set using each of these models, say pred_test1, pred_test2 and make a data frame called ensemble_test where these are the columns.
	# 4. Now make a model ensemble_model which is built using ensemble_train and predicts on ensemble_test. This is the final prediction.

	predictions <- data.frame(algo1.prediction = , algo2.prediction = , algo3.prediction = ,
								final.prediction = rep(0, nrow(cv)), actual.label = cv$target)

    getmode <- function(x) {
    unique.x <- unique(x)
    unique.x[which.max(tabulate(match(x, unique.x)))]
	}

	predictions$final.prediction <- apply(predictions, 1, getmode)

==============================================================================================================================

### Splitting data set randomly

	# sample.split balances partitions keeping in mind the outcome variable

		library("caTools")
		set.seed(10)
		split <- sample.split(data_frame$target, SplitRatio = 0.8)
		train <- subset(data_frame, split == TRUE)
		cv <- subset(data_frame, split == FALSE)

	# using sample

		set.seed(123)
		indices <- sample(2, nrow(data_frame), replace = T, prob = c(0.75, 0.25))
		train <- data_frame[indices == 1, ]
		cv <- data_frame[indices == 2, ]

	# caret

		train_indices <- createDataPartition(train$target, p = 0.75, list = FALSE)
		train <- Sonar[train_indices, ]
		cv  <- Sonar[-train_indices, ]

==============================================================================================================================

### Sampling Techniques

	# sampling is to be done in case of highly unbalanced class. Only training data has to be sampled.

		# ROSE

			library(ROSE)

			# oversampling
			oversampled_train_data <- ovun.sample(target ~ ., data = train, method = "over",
	                                    	N = 2*length(train$target[train$target == "class_with_more_observations"]),
	                                      	seed = 1)$data
			
			# undersampling
			undersampled_train_data <- ovun.sample(target ~ ., data = train, method = "under",
	                                    	N = 2*length(train$target[train$target == "class_with_less_observations"]),
	                                      	seed = 1)$data

			# mixed sampling
			mix_sampled_train_data <- ovun.sample(target ~ ., data = train, method = "both", p=0.5, 
	                                  		N=nrow(train),
	                                  		seed = 1)$data

			# ROSE (type of mixed sampling)
			rose_train_data <- ROSE(target ~ ., data = train, seed = 1)$data

		# SMOTE

			library(DMwR)
			smote_train_data <- SMOTE(target ~ ., data = train, perc.over = 100, perc.under = 200, k = 5)

			library(smotefamily)
			library(FNN)

			smote_train_data <- SMOTE(train[, !colnames(train) %in% c("target")], train$target, K = 5, dup_size = 0)

		# caret

			library(caret)

			set.seed(4)
			caret_train_date <- downSample(x = train[, !colnames(train) %in% c("target")], y = train$target)
			caret_train_date <- upSample(x = train[, !colnames(train) %in% c("target")], y = train$target)
			
			OR
			
			algo.control = trainControl(method = "cv", 
										number = 5,
										classProbs = T,
										summaryFunction = defaultSummary,  # twoClassSummary for binary classification
										verboseIter = T,
										allowParallel = T,
										sampling = "up/down/smote/rose"
										)

==============================================================================================================================

### One-Hot Encoding

		library(caret)

		dummies <- dummyVars(target ~ ., data = data_frame)
		temp <- as.data.frame(predict(dummies, data_frame))

==============================================================================================================================

### Handling missing values

	# To be run only on variables having missing value. For convinience run on every variable except for dependent variable.
		
		# check missing values
		colSums(is.na(data_frame))*100/nrow(data_frame)
		sapply(data_frame, function(x) sum(is.na(x))*100/length(x))

		library("Hmisc")
		df_imputed <- aregImpute(~ x1 + x2 + x3, data = data_frame, n.impute = 5)
		df_imputed
		df_imputed$imputed$x1

		library("missForest")
		df_imputed <- missForest(data_frame)
		df_imputed$ximp
		
		library("mice")
		md.pattern(data_frame)

		set.seed(10)
		imputed <- complete(mice(data_frame))
		data_frame$var <- imputed$var

		library(DMwR)
		inputData <- knnImputation(inputData)

		knn.impute(data, k = 10, cat.var = 1:ncol(data), to.impute = 1:nrow(data), using = 1:nrow(data))

==============================================================================================================================

### Removing Variables
		
		data_frame <- data_frame[ , !(names(data_frame) %in% c("x1","x2","x3"))]
		data_frame$var <- NULL
		new_data_frame <- setdiff(names(data_frame), c("x1","x2","x3"))  # check if setdiff() or data_frame[, setdiff()]

==============================================================================================================================

### Correlation and scatterplot matrix
	
	# correlation matrix

		library(corrplot)
		
		numeric <- sapply(data_frame, is.numeric)
		cordata <- data_frame[ ,numeric]
		cordata <- na.omit(cordata)
		cor_matrix <- cor(cordata) # to see correlation table
		cor_matrix
		corrplot(cor_matrix, method = "square", type = "lower") # to visualize correlation matrix

	# scatterplot matrix

		pairs(cordata) # scatterplot matrix

==============================================================================================================================

### Binning
	
		library("Hmisc")

		bins <- cut2(data_frame$variable, g = number_of_bins) # cuts the variable on basis of quantile.

==============================================================================================================================

### Check constant factors

		logical <- sapply(data_frame, function(x) is.factor(x))
		factor.variables <- data_frame[ , names(which(logical == TRUE))]
		ifelse(factor.variables <- sapply(factor.variables, function(x) length(levels(x))) == 1,"constant_factor","0")
		table(factor.variables)

==============================================================================================================================

### Boruta Analysis to find importance of variable in a data set
	
	library(Boruta)

		set.seed(13)
		boruta.model <- Boruta(targetVariable ~ ., data = train, maxRuns=101, doTrace=0)
		summary(boruta.model)
		boruta.cor.matrix <- attStats(boruta.model)
		important.features <- names(data_frame)[boruta.model$finalDecision!="Rejected"]
		important.features

==============================================================================================================================

### R functions
	
		which.max(vector)
		which.min(vector)
		which(vector == "value")
		match("value",var)
		subset(data_frame, condition)
		rep(x, times)
		rnorm(n, mean = 0, sd = 1)
		sample(vector/number, size, replace = FALSE, prob = NULL) # trainSmall <- train[sample(nrow(train), x), ]

		apply(array/matrix/data_frame, 1/2, function, ...) # need to specify row/column.
		lapply(list/vector/data_frame, function, ...) # work on columns. return list.
		sapply(list/vector/data_frame, function, ...) # work on columns. return vector if possible else matrix else list.
		tapply(function.applied.to.this.var, result.displayed.according.to.this.var, function)

		rowSums(vector, na.rm = T)
		colSums(vector, na.rm = T)

==============================================================================================================================

### dplyr
		
		library("dplyr")

		select(data_frame, column_names)
		filter(data_frame, condition)
		arrange(data_frame, desc(factor_variable)) # ascending by default
		mutate(data_frame, new_variable_name = equation)
		group_by(data_frame, factor_variable)
		summarize(data_frame, newVarName =  function()) # group_by(factorVariable) %>% summarize(count = n()) used commonly.
		count(data_frame, variable) # works similar table
		top_n(20, data_frame) # for top 20 rows

		piping : data_frame %>% operation1 %>% operation2 and so on...

==============================================================================================================================

### tidyr

		library(tidyr)

		separate(data_frame, variable.to.separate, c("new","names"), sep = ", ") # separate rohtak, haryana. Quotations important.
		
		unite(data_frame, new.variable, c(var1, var2), sep = " ") # combine first name and last name into name. No quotations for new variable names.

		gather(data_frame, new.factor.var, new.numerical.var, column.start.name:column.end.name) %>%
				filter(new.numerical.var == 1) %>% 
				select(-new.numerical.var) # wide to long

		spread(data_frame, categorical.var, corresponding.numerical.var) # long to wide

==============================================================================================================================

### Date and Time
	
		data_frame$date <- as.POSIXct(strptime(variable, format = "")) # format can be - "%d/%m/%Y %H:%M:%S"
		data_frame$day <- as.integer(format(data_frame$date, "%d")) # day
		data_frame$month <- as.factor(format(data_frame$date, "%B")) # month
		data_frame$year <- as.integer(format(data_frame$date, "%Y")) # year
		data_frame$weekday <- as.factor(format(data_frame$date, "%A")) # weekday
		data_frame$hour <- as.integer(format(data_frame$date, "%H")) # hour
		data_frame$minute <- as.integer(format(data_frame$date, "%M")) # minute

==============================================================================================================================

### Efficient Code
	
	# Never grow a vector. Always use matrices instead of data frames whenever possible.

		# check time taken by code
		system.time(
			# code
			)

	library(microbenchmark)

		# evaluate and compare time taken by multiple expressions expressions
		microbenchmark(rnorm(1000), 1:1000, times = )  # times to run each expression

	library(benchmarkme)

		# time taken to read and write csv files
		benchmark_io(runs = , size = )  # size in MBs

	library(profvis)

		# show memory used and CPU time taken by code
		profvis({
			# code
			})

	library(parallel)

		# Print no_of_cores
		detectCores()

		# assign cores to use
		num_cores <- detectCores()

		# start cluster
		cluster <- makeCluster(num_cores)

		# optional: makeCluster does not include functions present in the envinronment.
		clusterExport(cluster, "function name as string")  # only pass function name within quotations.

		# code
		parApply(cluster, array/matrix/data_frame, 1/2, function, ...)
		parSapply(cluster, list/vector/data_frame, function, ...)
		parLapply(cluster, list/vector/data_frame, function, ...)

		# close cluster
		stopCluster(cluster)

==============================================================================================================================
